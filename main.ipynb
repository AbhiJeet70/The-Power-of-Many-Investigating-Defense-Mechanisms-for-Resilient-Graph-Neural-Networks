{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9kN01M2wBkEXHRisLYMkV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhiJeet70/The-Power-of-Many-Investigating-Defense-Mechanisms-for-Resilient-Graph-Neural-Networks/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nmg6ldxSccM6",
        "outputId": "b3b65fa7-a90c-49bf-f5dc-e29b98582ea2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Using device: cpu\n",
            "Dataset: Cora, Model: GCN, Baseline Accuracy: 87.80%\n",
            "Epoch 0, Reconstruction Loss: 0.0128\n",
            "Epoch 10, Reconstruction Loss: 0.0124\n",
            "Epoch 20, Reconstruction Loss: 0.0122\n",
            "Epoch 30, Reconstruction Loss: 0.0120\n",
            "Epoch 40, Reconstruction Loss: 0.0119\n",
            "Epoch 50, Reconstruction Loss: 0.0118\n",
            "Epoch 60, Reconstruction Loss: 0.0117\n",
            "Epoch 70, Reconstruction Loss: 0.0116\n",
            "Epoch 80, Reconstruction Loss: 0.0115\n",
            "Epoch 90, Reconstruction Loss: 0.0114\n",
            "Epoch 0, Loss: 0.1147\n",
            "Epoch 10, Loss: 0.1050\n",
            "Epoch 20, Loss: 0.0991\n",
            "Epoch 30, Loss: 0.1002\n",
            "Epoch 40, Loss: 0.0909\n",
            "Epoch 50, Loss: 0.0879\n",
            "Epoch 60, Loss: 0.0830\n",
            "Epoch 70, Loss: 0.0823\n",
            "Epoch 80, Loss: 0.0770\n",
            "Epoch 90, Loss: 0.0758\n",
            "Dataset: Cora, Model: GCN, Attack: SBA-Samp, Defense: None - ASR: 100.00%, Clean Accuracy: 87.80%\n",
            "Dataset: Cora, Model: GCN, Attack: SBA-Samp, Defense: DOMINANT - ASR: 100.00%, Clean Accuracy: 78.37%\n",
            "Dataset: Cora, Model: GCN, Attack: SBA-Samp, Defense: Prune - ASR: 90.00%, Clean Accuracy: 79.85%\n",
            "Dataset: Cora, Model: GCN, Attack: SBA-Samp, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 44.36%\n",
            "Epoch 0, Loss: 0.0716\n",
            "Epoch 10, Loss: 0.0703\n",
            "Epoch 20, Loss: 0.0681\n",
            "Epoch 30, Loss: 0.0649\n",
            "Epoch 40, Loss: 0.0625\n",
            "Epoch 50, Loss: 0.0604\n",
            "Epoch 60, Loss: 0.0601\n",
            "Epoch 70, Loss: 0.0590\n",
            "Epoch 80, Loss: 0.0564\n",
            "Epoch 90, Loss: 0.0543\n",
            "Dataset: Cora, Model: GCN, Attack: SBA-Gen, Defense: None - ASR: 100.00%, Clean Accuracy: 86.51%\n",
            "Dataset: Cora, Model: GCN, Attack: SBA-Gen, Defense: DOMINANT - ASR: 100.00%, Clean Accuracy: 77.82%\n",
            "Dataset: Cora, Model: GCN, Attack: SBA-Gen, Defense: Prune - ASR: 90.00%, Clean Accuracy: 78.74%\n",
            "Dataset: Cora, Model: GCN, Attack: SBA-Gen, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 43.44%\n",
            "Epoch 0, Loss: 0.0594\n",
            "Epoch 10, Loss: 0.0517\n",
            "Epoch 20, Loss: 0.0459\n",
            "Epoch 30, Loss: 0.0503\n",
            "Epoch 40, Loss: 0.0489\n",
            "Epoch 50, Loss: 0.0438\n",
            "Epoch 60, Loss: 0.0443\n",
            "Epoch 70, Loss: 0.0472\n",
            "Epoch 80, Loss: 0.0418\n",
            "Epoch 90, Loss: 0.0425\n",
            "Dataset: Cora, Model: GCN, Attack: GTA, Defense: None - ASR: 100.00%, Clean Accuracy: 87.80%\n",
            "Dataset: Cora, Model: GCN, Attack: GTA, Defense: DOMINANT - ASR: 90.00%, Clean Accuracy: 79.30%\n",
            "Dataset: Cora, Model: GCN, Attack: GTA, Defense: Prune - ASR: 100.00%, Clean Accuracy: 77.82%\n",
            "Dataset: Cora, Model: GCN, Attack: GTA, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 43.44%\n",
            "Epoch 0, Loss: 0.9197\n",
            "Epoch 10, Loss: 0.2978\n",
            "Epoch 20, Loss: 0.0785\n",
            "Epoch 30, Loss: 0.0614\n",
            "Epoch 40, Loss: 0.0497\n",
            "Epoch 50, Loss: 0.0462\n",
            "Epoch 60, Loss: 0.0436\n",
            "Epoch 70, Loss: 0.0403\n",
            "Epoch 80, Loss: 0.0412\n",
            "Epoch 90, Loss: 0.0396\n",
            "Dataset: Cora, Model: GCN, Attack: UGBA, Defense: None - ASR: 100.00%, Clean Accuracy: 87.43%\n",
            "Dataset: Cora, Model: GCN, Attack: UGBA, Defense: DOMINANT - ASR: 80.00%, Clean Accuracy: 76.89%\n",
            "Dataset: Cora, Model: GCN, Attack: UGBA, Defense: Prune - ASR: 80.00%, Clean Accuracy: 77.26%\n",
            "Dataset: Cora, Model: GCN, Attack: UGBA, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 43.81%\n",
            "Epoch 0, Loss: 0.0382\n",
            "Epoch 10, Loss: 0.0399\n",
            "Epoch 20, Loss: 0.0357\n",
            "Epoch 30, Loss: 0.0359\n",
            "Epoch 40, Loss: 0.0362\n",
            "Epoch 50, Loss: 0.0363\n",
            "Epoch 60, Loss: 0.0363\n",
            "Epoch 70, Loss: 0.0332\n",
            "Epoch 80, Loss: 0.0349\n",
            "Epoch 90, Loss: 0.0322\n",
            "Dataset: Cora, Model: GCN, Attack: DPGBA, Defense: None - ASR: 100.00%, Clean Accuracy: 87.06%\n",
            "Dataset: Cora, Model: GCN, Attack: DPGBA, Defense: DOMINANT - ASR: 100.00%, Clean Accuracy: 79.11%\n",
            "Dataset: Cora, Model: GCN, Attack: DPGBA, Defense: Prune - ASR: 80.00%, Clean Accuracy: 76.71%\n",
            "Dataset: Cora, Model: GCN, Attack: DPGBA, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 43.25%\n",
            "Dataset: Cora, Model: GraphSage, Baseline Accuracy: 88.72%\n",
            "Epoch 0, Reconstruction Loss: 0.0128\n",
            "Epoch 10, Reconstruction Loss: 0.0124\n",
            "Epoch 20, Reconstruction Loss: 0.0121\n",
            "Epoch 30, Reconstruction Loss: 0.0120\n",
            "Epoch 40, Reconstruction Loss: 0.0119\n",
            "Epoch 50, Reconstruction Loss: 0.0118\n",
            "Epoch 60, Reconstruction Loss: 0.0116\n",
            "Epoch 70, Reconstruction Loss: 0.0115\n",
            "Epoch 80, Reconstruction Loss: 0.0114\n",
            "Epoch 90, Reconstruction Loss: 0.0112\n",
            "Epoch 0, Loss: 0.0149\n",
            "Epoch 10, Loss: 0.0132\n",
            "Epoch 20, Loss: 0.0128\n",
            "Epoch 30, Loss: 0.0100\n",
            "Epoch 40, Loss: 0.0096\n",
            "Epoch 50, Loss: 0.0077\n",
            "Epoch 60, Loss: 0.0070\n",
            "Epoch 70, Loss: 0.0063\n",
            "Epoch 80, Loss: 0.0064\n",
            "Epoch 90, Loss: 0.0052\n",
            "Dataset: Cora, Model: GraphSage, Attack: SBA-Samp, Defense: None - ASR: 100.00%, Clean Accuracy: 88.72%\n",
            "Dataset: Cora, Model: GraphSage, Attack: SBA-Samp, Defense: DOMINANT - ASR: 100.00%, Clean Accuracy: 80.59%\n",
            "Dataset: Cora, Model: GraphSage, Attack: SBA-Samp, Defense: Prune - ASR: 100.00%, Clean Accuracy: 74.31%\n",
            "Dataset: Cora, Model: GraphSage, Attack: SBA-Samp, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 40.67%\n",
            "Epoch 0, Loss: 0.0055\n",
            "Epoch 10, Loss: 0.0051\n",
            "Epoch 20, Loss: 0.0057\n",
            "Epoch 30, Loss: 0.0041\n",
            "Epoch 40, Loss: 0.0042\n",
            "Epoch 50, Loss: 0.0049\n",
            "Epoch 60, Loss: 0.0035\n",
            "Epoch 70, Loss: 0.0036\n",
            "Epoch 80, Loss: 0.0045\n",
            "Epoch 90, Loss: 0.0030\n",
            "Dataset: Cora, Model: GraphSage, Attack: SBA-Gen, Defense: None - ASR: 100.00%, Clean Accuracy: 88.17%\n",
            "Dataset: Cora, Model: GraphSage, Attack: SBA-Gen, Defense: DOMINANT - ASR: 100.00%, Clean Accuracy: 81.15%\n",
            "Dataset: Cora, Model: GraphSage, Attack: SBA-Gen, Defense: Prune - ASR: 100.00%, Clean Accuracy: 74.12%\n",
            "Dataset: Cora, Model: GraphSage, Attack: SBA-Gen, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 41.04%\n",
            "Epoch 0, Loss: 0.0096\n",
            "Epoch 10, Loss: 0.0054\n",
            "Epoch 20, Loss: 0.0038\n",
            "Epoch 30, Loss: 0.0037\n",
            "Epoch 40, Loss: 0.0032\n",
            "Epoch 50, Loss: 0.0033\n",
            "Epoch 60, Loss: 0.0032\n",
            "Epoch 70, Loss: 0.0028\n",
            "Epoch 80, Loss: 0.0020\n",
            "Epoch 90, Loss: 0.0023\n",
            "Dataset: Cora, Model: GraphSage, Attack: GTA, Defense: None - ASR: 100.00%, Clean Accuracy: 87.99%\n",
            "Dataset: Cora, Model: GraphSage, Attack: GTA, Defense: DOMINANT - ASR: 100.00%, Clean Accuracy: 79.85%\n",
            "Dataset: Cora, Model: GraphSage, Attack: GTA, Defense: Prune - ASR: 100.00%, Clean Accuracy: 73.94%\n",
            "Dataset: Cora, Model: GraphSage, Attack: GTA, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 41.04%\n",
            "Epoch 0, Loss: 0.9657\n",
            "Epoch 10, Loss: 0.3148\n",
            "Epoch 20, Loss: 0.1429\n",
            "Epoch 30, Loss: 0.0227\n",
            "Epoch 40, Loss: 0.0158\n",
            "Epoch 50, Loss: 0.0103\n",
            "Epoch 60, Loss: 0.0058\n",
            "Epoch 70, Loss: 0.0066\n",
            "Epoch 80, Loss: 0.0051\n",
            "Epoch 90, Loss: 0.0043\n",
            "Dataset: Cora, Model: GraphSage, Attack: UGBA, Defense: None - ASR: 100.00%, Clean Accuracy: 88.54%\n",
            "Dataset: Cora, Model: GraphSage, Attack: UGBA, Defense: DOMINANT - ASR: 80.00%, Clean Accuracy: 78.74%\n",
            "Dataset: Cora, Model: GraphSage, Attack: UGBA, Defense: Prune - ASR: 100.00%, Clean Accuracy: 75.05%\n",
            "Dataset: Cora, Model: GraphSage, Attack: UGBA, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 41.22%\n",
            "Epoch 0, Loss: 0.0033\n",
            "Epoch 10, Loss: 0.0040\n",
            "Epoch 20, Loss: 0.0031\n",
            "Epoch 30, Loss: 0.0027\n",
            "Epoch 40, Loss: 0.0032\n",
            "Epoch 50, Loss: 0.0030\n",
            "Epoch 60, Loss: 0.0022\n",
            "Epoch 70, Loss: 0.0024\n",
            "Epoch 80, Loss: 0.0022\n",
            "Epoch 90, Loss: 0.0021\n",
            "Dataset: Cora, Model: GraphSage, Attack: DPGBA, Defense: None - ASR: 100.00%, Clean Accuracy: 88.54%\n",
            "Dataset: Cora, Model: GraphSage, Attack: DPGBA, Defense: DOMINANT - ASR: 100.00%, Clean Accuracy: 81.15%\n",
            "Dataset: Cora, Model: GraphSage, Attack: DPGBA, Defense: Prune - ASR: 90.00%, Clean Accuracy: 74.68%\n",
            "Dataset: Cora, Model: GraphSage, Attack: DPGBA, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 41.22%\n",
            "Dataset: Cora, Model: GAT, Baseline Accuracy: 87.06%\n",
            "Epoch 0, Reconstruction Loss: 0.0128\n",
            "Epoch 10, Reconstruction Loss: 0.0124\n",
            "Epoch 20, Reconstruction Loss: 0.0121\n",
            "Epoch 30, Reconstruction Loss: 0.0120\n",
            "Epoch 40, Reconstruction Loss: 0.0119\n",
            "Epoch 50, Reconstruction Loss: 0.0118\n",
            "Epoch 60, Reconstruction Loss: 0.0117\n",
            "Epoch 70, Reconstruction Loss: 0.0116\n",
            "Epoch 80, Reconstruction Loss: 0.0114\n",
            "Epoch 90, Reconstruction Loss: 0.0113\n",
            "Epoch 0, Loss: 0.0369\n",
            "Epoch 10, Loss: 0.0188\n",
            "Epoch 20, Loss: 0.0137\n",
            "Epoch 30, Loss: 0.0105\n",
            "Epoch 40, Loss: 0.0103\n",
            "Epoch 50, Loss: 0.0119\n",
            "Epoch 60, Loss: 0.0099\n",
            "Epoch 70, Loss: 0.0093\n",
            "Epoch 80, Loss: 0.0089\n",
            "Epoch 90, Loss: 0.0098\n",
            "Dataset: Cora, Model: GAT, Attack: SBA-Samp, Defense: None - ASR: 100.00%, Clean Accuracy: 86.14%\n",
            "Dataset: Cora, Model: GAT, Attack: SBA-Samp, Defense: DOMINANT - ASR: 100.00%, Clean Accuracy: 77.82%\n",
            "Dataset: Cora, Model: GAT, Attack: SBA-Samp, Defense: Prune - ASR: 90.00%, Clean Accuracy: 81.52%\n",
            "Dataset: Cora, Model: GAT, Attack: SBA-Samp, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 44.18%\n",
            "Epoch 0, Loss: 0.0095\n",
            "Epoch 10, Loss: 0.0086\n",
            "Epoch 20, Loss: 0.0076\n",
            "Epoch 30, Loss: 0.0079\n",
            "Epoch 40, Loss: 0.0069\n",
            "Epoch 50, Loss: 0.0069\n",
            "Epoch 60, Loss: 0.0066\n",
            "Epoch 70, Loss: 0.0074\n",
            "Epoch 80, Loss: 0.0071\n",
            "Epoch 90, Loss: 0.0067\n",
            "Dataset: Cora, Model: GAT, Attack: SBA-Gen, Defense: None - ASR: 100.00%, Clean Accuracy: 86.51%\n",
            "Dataset: Cora, Model: GAT, Attack: SBA-Gen, Defense: DOMINANT - ASR: 100.00%, Clean Accuracy: 78.93%\n",
            "Dataset: Cora, Model: GAT, Attack: SBA-Gen, Defense: Prune - ASR: 90.00%, Clean Accuracy: 80.41%\n",
            "Dataset: Cora, Model: GAT, Attack: SBA-Gen, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 43.62%\n",
            "Epoch 0, Loss: 0.0073\n",
            "Epoch 10, Loss: 0.0071\n",
            "Epoch 20, Loss: 0.0061\n",
            "Epoch 30, Loss: 0.0070\n",
            "Epoch 40, Loss: 0.0066\n",
            "Epoch 50, Loss: 0.0057\n",
            "Epoch 60, Loss: 0.0058\n",
            "Epoch 70, Loss: 0.0053\n",
            "Epoch 80, Loss: 0.0061\n",
            "Epoch 90, Loss: 0.0061\n",
            "Dataset: Cora, Model: GAT, Attack: GTA, Defense: None - ASR: 100.00%, Clean Accuracy: 86.51%\n",
            "Dataset: Cora, Model: GAT, Attack: GTA, Defense: DOMINANT - ASR: 100.00%, Clean Accuracy: 77.26%\n",
            "Dataset: Cora, Model: GAT, Attack: GTA, Defense: Prune - ASR: 100.00%, Clean Accuracy: 80.04%\n",
            "Dataset: Cora, Model: GAT, Attack: GTA, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 43.25%\n",
            "Epoch 0, Loss: 0.8139\n",
            "Epoch 10, Loss: 3.1959\n",
            "Epoch 20, Loss: 1.9457\n",
            "Epoch 30, Loss: 0.5918\n",
            "Epoch 40, Loss: 0.7071\n",
            "Epoch 50, Loss: 0.7314\n",
            "Epoch 60, Loss: 0.4420\n",
            "Epoch 70, Loss: 0.8591\n",
            "Epoch 80, Loss: 1.0678\n",
            "Epoch 90, Loss: 0.6289\n",
            "Dataset: Cora, Model: GAT, Attack: UGBA, Defense: None - ASR: 100.00%, Clean Accuracy: 85.95%\n",
            "Dataset: Cora, Model: GAT, Attack: UGBA, Defense: DOMINANT - ASR: 90.00%, Clean Accuracy: 74.49%\n",
            "Dataset: Cora, Model: GAT, Attack: UGBA, Defense: Prune - ASR: 80.00%, Clean Accuracy: 78.00%\n",
            "Dataset: Cora, Model: GAT, Attack: UGBA, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 43.07%\n",
            "Epoch 0, Loss: 0.0940\n",
            "Epoch 10, Loss: 0.0825\n",
            "Epoch 20, Loss: 0.0896\n",
            "Epoch 30, Loss: 0.0644\n",
            "Epoch 40, Loss: 0.0646\n",
            "Epoch 50, Loss: 0.0644\n",
            "Epoch 60, Loss: 0.0648\n",
            "Epoch 70, Loss: 0.0670\n",
            "Epoch 80, Loss: 0.0420\n",
            "Epoch 90, Loss: 0.0666\n",
            "Dataset: Cora, Model: GAT, Attack: DPGBA, Defense: None - ASR: 100.00%, Clean Accuracy: 86.32%\n",
            "Dataset: Cora, Model: GAT, Attack: DPGBA, Defense: DOMINANT - ASR: 100.00%, Clean Accuracy: 78.37%\n",
            "Dataset: Cora, Model: GAT, Attack: DPGBA, Defense: Prune - ASR: 70.00%, Clean Accuracy: 78.56%\n",
            "Dataset: Cora, Model: GAT, Attack: DPGBA, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 43.44%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: PubMed, Model: GCN, Baseline Accuracy: 85.47%\n",
            "Epoch 0, Reconstruction Loss: 0.0003\n",
            "Epoch 10, Reconstruction Loss: 0.0003\n",
            "Epoch 20, Reconstruction Loss: 0.0003\n",
            "Epoch 30, Reconstruction Loss: 0.0003\n",
            "Epoch 40, Reconstruction Loss: 0.0003\n",
            "Epoch 50, Reconstruction Loss: 0.0003\n",
            "Epoch 60, Reconstruction Loss: 0.0003\n",
            "Epoch 70, Reconstruction Loss: 0.0003\n",
            "Epoch 80, Reconstruction Loss: 0.0003\n",
            "Epoch 90, Reconstruction Loss: 0.0003\n",
            "Epoch 0, Loss: 0.3777\n",
            "Epoch 10, Loss: 0.3731\n",
            "Epoch 20, Loss: 0.3680\n",
            "Epoch 30, Loss: 0.3637\n",
            "Epoch 40, Loss: 0.3615\n",
            "Epoch 50, Loss: 0.3582\n",
            "Epoch 60, Loss: 0.3553\n",
            "Epoch 70, Loss: 0.3519\n",
            "Epoch 80, Loss: 0.3489\n",
            "Epoch 90, Loss: 0.3469\n",
            "Dataset: PubMed, Model: GCN, Attack: SBA-Samp, Defense: None - ASR: 95.00%, Clean Accuracy: 86.05%\n",
            "Dataset: PubMed, Model: GCN, Attack: SBA-Samp, Defense: DOMINANT - ASR: 87.50%, Clean Accuracy: 76.82%\n",
            "Dataset: PubMed, Model: GCN, Attack: SBA-Samp, Defense: Prune - ASR: 92.50%, Clean Accuracy: 82.07%\n",
            "Dataset: PubMed, Model: GCN, Attack: SBA-Samp, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 42.81%\n",
            "Epoch 0, Loss: 0.3417\n",
            "Epoch 10, Loss: 0.3392\n",
            "Epoch 20, Loss: 0.3378\n",
            "Epoch 30, Loss: 0.3344\n",
            "Epoch 40, Loss: 0.3331\n",
            "Epoch 50, Loss: 0.3315\n",
            "Epoch 60, Loss: 0.3298\n",
            "Epoch 70, Loss: 0.3286\n",
            "Epoch 80, Loss: 0.3259\n",
            "Epoch 90, Loss: 0.3247\n",
            "Dataset: PubMed, Model: GCN, Attack: SBA-Gen, Defense: None - ASR: 95.00%, Clean Accuracy: 86.46%\n",
            "Dataset: PubMed, Model: GCN, Attack: SBA-Gen, Defense: DOMINANT - ASR: 87.50%, Clean Accuracy: 76.97%\n",
            "Dataset: PubMed, Model: GCN, Attack: SBA-Gen, Defense: Prune - ASR: 92.50%, Clean Accuracy: 82.45%\n",
            "Dataset: PubMed, Model: GCN, Attack: SBA-Gen, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 43.04%\n",
            "Epoch 0, Loss: 0.3231\n",
            "Epoch 10, Loss: 0.3220\n",
            "Epoch 20, Loss: 0.3204\n",
            "Epoch 30, Loss: 0.3158\n",
            "Epoch 40, Loss: 0.3176\n",
            "Epoch 50, Loss: 0.3149\n",
            "Epoch 60, Loss: 0.3138\n",
            "Epoch 70, Loss: 0.3115\n",
            "Epoch 80, Loss: 0.3108\n",
            "Epoch 90, Loss: 0.3106\n",
            "Dataset: PubMed, Model: GCN, Attack: GTA, Defense: None - ASR: 95.00%, Clean Accuracy: 86.48%\n",
            "Dataset: PubMed, Model: GCN, Attack: GTA, Defense: DOMINANT - ASR: 70.00%, Clean Accuracy: 77.07%\n",
            "Dataset: PubMed, Model: GCN, Attack: GTA, Defense: Prune - ASR: 60.00%, Clean Accuracy: 82.42%\n",
            "Dataset: PubMed, Model: GCN, Attack: GTA, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 43.27%\n",
            "Epoch 0, Loss: 0.3460\n",
            "Epoch 10, Loss: 0.3376\n",
            "Epoch 20, Loss: 0.3188\n",
            "Epoch 30, Loss: 0.3105\n",
            "Epoch 40, Loss: 0.3107\n",
            "Epoch 50, Loss: 0.3086\n",
            "Epoch 60, Loss: 0.3027\n",
            "Epoch 70, Loss: 0.3018\n",
            "Epoch 80, Loss: 0.2992\n",
            "Epoch 90, Loss: 0.2980\n",
            "Dataset: PubMed, Model: GCN, Attack: UGBA, Defense: None - ASR: 95.00%, Clean Accuracy: 86.63%\n",
            "Dataset: PubMed, Model: GCN, Attack: UGBA, Defense: DOMINANT - ASR: 85.00%, Clean Accuracy: 77.63%\n",
            "Dataset: PubMed, Model: GCN, Attack: UGBA, Defense: Prune - ASR: 92.50%, Clean Accuracy: 82.68%\n",
            "Dataset: PubMed, Model: GCN, Attack: UGBA, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 43.11%\n",
            "Epoch 0, Loss: 0.2969\n",
            "Epoch 10, Loss: 0.2963\n",
            "Epoch 20, Loss: 0.2976\n",
            "Epoch 30, Loss: 0.2953\n",
            "Epoch 40, Loss: 0.2929\n",
            "Epoch 50, Loss: 0.2933\n",
            "Epoch 60, Loss: 0.2922\n",
            "Epoch 70, Loss: 0.2883\n",
            "Epoch 80, Loss: 0.2898\n",
            "Epoch 90, Loss: 0.2867\n",
            "Dataset: PubMed, Model: GCN, Attack: DPGBA, Defense: None - ASR: 95.00%, Clean Accuracy: 86.91%\n",
            "Dataset: PubMed, Model: GCN, Attack: DPGBA, Defense: DOMINANT - ASR: 85.00%, Clean Accuracy: 77.43%\n",
            "Dataset: PubMed, Model: GCN, Attack: DPGBA, Defense: Prune - ASR: 90.00%, Clean Accuracy: 82.15%\n",
            "Dataset: PubMed, Model: GCN, Attack: DPGBA, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 43.17%\n",
            "Dataset: PubMed, Model: GraphSage, Baseline Accuracy: 86.51%\n",
            "Epoch 0, Reconstruction Loss: 0.0003\n",
            "Epoch 10, Reconstruction Loss: 0.0003\n",
            "Epoch 20, Reconstruction Loss: 0.0003\n",
            "Epoch 30, Reconstruction Loss: 0.0003\n",
            "Epoch 40, Reconstruction Loss: 0.0003\n",
            "Epoch 50, Reconstruction Loss: 0.0003\n",
            "Epoch 60, Reconstruction Loss: 0.0003\n",
            "Epoch 70, Reconstruction Loss: 0.0003\n",
            "Epoch 80, Reconstruction Loss: 0.0003\n",
            "Epoch 90, Reconstruction Loss: 0.0003\n",
            "Epoch 0, Loss: 0.2868\n",
            "Epoch 10, Loss: 0.2799\n",
            "Epoch 20, Loss: 0.2731\n",
            "Epoch 30, Loss: 0.2727\n",
            "Epoch 40, Loss: 0.2668\n",
            "Epoch 50, Loss: 0.2631\n",
            "Epoch 60, Loss: 0.2569\n",
            "Epoch 70, Loss: 0.2512\n",
            "Epoch 80, Loss: 0.2478\n",
            "Epoch 90, Loss: 0.2448\n",
            "Dataset: PubMed, Model: GraphSage, Attack: SBA-Samp, Defense: None - ASR: 92.50%, Clean Accuracy: 86.61%\n",
            "Dataset: PubMed, Model: GraphSage, Attack: SBA-Samp, Defense: DOMINANT - ASR: 85.00%, Clean Accuracy: 77.45%\n",
            "Dataset: PubMed, Model: GraphSage, Attack: SBA-Samp, Defense: Prune - ASR: 85.00%, Clean Accuracy: 82.91%\n",
            "Dataset: PubMed, Model: GraphSage, Attack: SBA-Samp, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 43.55%\n",
            "Epoch 0, Loss: 0.2515\n",
            "Epoch 10, Loss: 0.2368\n",
            "Epoch 20, Loss: 0.2328\n",
            "Epoch 30, Loss: 0.2303\n",
            "Epoch 40, Loss: 0.2257\n",
            "Epoch 50, Loss: 0.2231\n",
            "Epoch 60, Loss: 0.2196\n",
            "Epoch 70, Loss: 0.2165\n",
            "Epoch 80, Loss: 0.2130\n",
            "Epoch 90, Loss: 0.2099\n",
            "Dataset: PubMed, Model: GraphSage, Attack: SBA-Gen, Defense: None - ASR: 92.50%, Clean Accuracy: 87.27%\n",
            "Dataset: PubMed, Model: GraphSage, Attack: SBA-Gen, Defense: DOMINANT - ASR: 85.00%, Clean Accuracy: 77.94%\n",
            "Dataset: PubMed, Model: GraphSage, Attack: SBA-Gen, Defense: Prune - ASR: 90.00%, Clean Accuracy: 82.98%\n",
            "Dataset: PubMed, Model: GraphSage, Attack: SBA-Gen, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 43.57%\n",
            "Epoch 0, Loss: 0.2468\n",
            "Epoch 10, Loss: 0.2171\n",
            "Epoch 20, Loss: 0.2083\n",
            "Epoch 30, Loss: 0.2046\n",
            "Epoch 40, Loss: 0.1986\n",
            "Epoch 50, Loss: 0.1985\n",
            "Epoch 60, Loss: 0.1948\n",
            "Epoch 70, Loss: 0.1930\n",
            "Epoch 80, Loss: 0.1902\n",
            "Epoch 90, Loss: 0.1874\n",
            "Dataset: PubMed, Model: GraphSage, Attack: GTA, Defense: None - ASR: 87.50%, Clean Accuracy: 87.65%\n",
            "Dataset: PubMed, Model: GraphSage, Attack: GTA, Defense: DOMINANT - ASR: 52.50%, Clean Accuracy: 78.11%\n",
            "Dataset: PubMed, Model: GraphSage, Attack: GTA, Defense: Prune - ASR: 65.00%, Clean Accuracy: 83.69%\n",
            "Dataset: PubMed, Model: GraphSage, Attack: GTA, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 44.08%\n",
            "Epoch 0, Loss: 0.2975\n",
            "Epoch 10, Loss: 0.1971\n",
            "Epoch 20, Loss: 0.1846\n",
            "Epoch 30, Loss: 0.1819\n",
            "Epoch 40, Loss: 0.1800\n",
            "Epoch 50, Loss: 0.1751\n",
            "Epoch 60, Loss: 0.1736\n",
            "Epoch 70, Loss: 0.1729\n",
            "Epoch 80, Loss: 0.1700\n",
            "Epoch 90, Loss: 0.1688\n",
            "Dataset: PubMed, Model: GraphSage, Attack: UGBA, Defense: None - ASR: 95.00%, Clean Accuracy: 87.57%\n",
            "Dataset: PubMed, Model: GraphSage, Attack: UGBA, Defense: DOMINANT - ASR: 87.50%, Clean Accuracy: 78.70%\n",
            "Dataset: PubMed, Model: GraphSage, Attack: UGBA, Defense: Prune - ASR: 92.50%, Clean Accuracy: 80.09%\n",
            "Dataset: PubMed, Model: GraphSage, Attack: UGBA, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 42.30%\n",
            "Epoch 0, Loss: 0.1700\n",
            "Epoch 10, Loss: 0.1664\n",
            "Epoch 20, Loss: 0.1629\n",
            "Epoch 30, Loss: 0.1580\n",
            "Epoch 40, Loss: 0.1589\n",
            "Epoch 50, Loss: 0.1560\n",
            "Epoch 60, Loss: 0.1552\n",
            "Epoch 70, Loss: 0.1521\n",
            "Epoch 80, Loss: 0.1512\n",
            "Epoch 90, Loss: 0.1521\n",
            "Dataset: PubMed, Model: GraphSage, Attack: DPGBA, Defense: None - ASR: 97.50%, Clean Accuracy: 87.42%\n",
            "Dataset: PubMed, Model: GraphSage, Attack: DPGBA, Defense: DOMINANT - ASR: 85.00%, Clean Accuracy: 78.29%\n",
            "Dataset: PubMed, Model: GraphSage, Attack: DPGBA, Defense: Prune - ASR: 90.00%, Clean Accuracy: 80.75%\n",
            "Dataset: PubMed, Model: GraphSage, Attack: DPGBA, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 42.68%\n",
            "Dataset: PubMed, Model: GAT, Baseline Accuracy: 86.69%\n",
            "Epoch 0, Reconstruction Loss: 0.0003\n",
            "Epoch 10, Reconstruction Loss: 0.0003\n",
            "Epoch 20, Reconstruction Loss: 0.0003\n",
            "Epoch 30, Reconstruction Loss: 0.0003\n",
            "Epoch 40, Reconstruction Loss: 0.0003\n",
            "Epoch 50, Reconstruction Loss: 0.0003\n",
            "Epoch 60, Reconstruction Loss: 0.0003\n",
            "Epoch 70, Reconstruction Loss: 0.0003\n",
            "Epoch 80, Reconstruction Loss: 0.0003\n",
            "Epoch 90, Reconstruction Loss: 0.0003\n",
            "Epoch 0, Loss: 0.2753\n",
            "Epoch 10, Loss: 0.2686\n",
            "Epoch 20, Loss: 0.2626\n",
            "Epoch 30, Loss: 0.2570\n",
            "Epoch 40, Loss: 0.2489\n",
            "Epoch 50, Loss: 0.2429\n",
            "Epoch 60, Loss: 0.2360\n",
            "Epoch 70, Loss: 0.2282\n",
            "Epoch 80, Loss: 0.2209\n",
            "Epoch 90, Loss: 0.2144\n",
            "Dataset: PubMed, Model: GAT, Attack: SBA-Samp, Defense: None - ASR: 97.50%, Clean Accuracy: 87.37%\n",
            "Dataset: PubMed, Model: GAT, Attack: SBA-Samp, Defense: DOMINANT - ASR: 90.00%, Clean Accuracy: 78.04%\n",
            "Dataset: PubMed, Model: GAT, Attack: SBA-Samp, Defense: Prune - ASR: 90.00%, Clean Accuracy: 82.63%\n",
            "Dataset: PubMed, Model: GAT, Attack: SBA-Samp, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 43.29%\n",
            "Epoch 0, Loss: 0.2183\n",
            "Epoch 10, Loss: 0.2026\n",
            "Epoch 20, Loss: 0.1938\n",
            "Epoch 30, Loss: 0.1870\n",
            "Epoch 40, Loss: 0.1820\n",
            "Epoch 50, Loss: 0.1735\n",
            "Epoch 60, Loss: 0.1681\n",
            "Epoch 70, Loss: 0.1624\n",
            "Epoch 80, Loss: 0.1551\n",
            "Epoch 90, Loss: 0.1505\n",
            "Dataset: PubMed, Model: GAT, Attack: SBA-Gen, Defense: None - ASR: 100.00%, Clean Accuracy: 87.65%\n",
            "Dataset: PubMed, Model: GAT, Attack: SBA-Gen, Defense: DOMINANT - ASR: 90.00%, Clean Accuracy: 78.37%\n",
            "Dataset: PubMed, Model: GAT, Attack: SBA-Gen, Defense: Prune - ASR: 87.50%, Clean Accuracy: 83.36%\n",
            "Dataset: PubMed, Model: GAT, Attack: SBA-Gen, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 43.82%\n",
            "Epoch 0, Loss: 0.2086\n",
            "Epoch 10, Loss: 0.1511\n",
            "Epoch 20, Loss: 0.1445\n",
            "Epoch 30, Loss: 0.1371\n",
            "Epoch 40, Loss: 0.1300\n",
            "Epoch 50, Loss: 0.1250\n",
            "Epoch 60, Loss: 0.1204\n",
            "Epoch 70, Loss: 0.1145\n",
            "Epoch 80, Loss: 0.1083\n",
            "Epoch 90, Loss: 0.1065\n",
            "Dataset: PubMed, Model: GAT, Attack: GTA, Defense: None - ASR: 100.00%, Clean Accuracy: 87.50%\n",
            "Dataset: PubMed, Model: GAT, Attack: GTA, Defense: DOMINANT - ASR: 52.50%, Clean Accuracy: 77.91%\n",
            "Dataset: PubMed, Model: GAT, Attack: GTA, Defense: Prune - ASR: 62.50%, Clean Accuracy: 83.95%\n",
            "Dataset: PubMed, Model: GAT, Attack: GTA, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 44.31%\n",
            "Epoch 0, Loss: 1.3703\n",
            "Epoch 10, Loss: 2.8729\n",
            "Epoch 20, Loss: 1.1592\n",
            "Epoch 30, Loss: 0.4687\n",
            "Epoch 40, Loss: 0.1646\n",
            "Epoch 50, Loss: 0.1684\n",
            "Epoch 60, Loss: 0.1471\n",
            "Epoch 70, Loss: 0.1220\n",
            "Epoch 80, Loss: 0.1415\n",
            "Epoch 90, Loss: 0.1490\n",
            "Dataset: PubMed, Model: GAT, Attack: UGBA, Defense: None - ASR: 100.00%, Clean Accuracy: 87.47%\n",
            "Dataset: PubMed, Model: GAT, Attack: UGBA, Defense: DOMINANT - ASR: 87.50%, Clean Accuracy: 78.57%\n",
            "Dataset: PubMed, Model: GAT, Attack: UGBA, Defense: Prune - ASR: 90.00%, Clean Accuracy: 84.05%\n",
            "Dataset: PubMed, Model: GAT, Attack: UGBA, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 44.28%\n",
            "Epoch 0, Loss: 0.1153\n",
            "Epoch 10, Loss: 0.1117\n",
            "Epoch 20, Loss: 0.1114\n",
            "Epoch 30, Loss: 0.1081\n",
            "Epoch 40, Loss: 0.1069\n",
            "Epoch 50, Loss: 0.1045\n",
            "Epoch 60, Loss: 0.1069\n",
            "Epoch 70, Loss: 0.1034\n",
            "Epoch 80, Loss: 0.1018\n",
            "Epoch 90, Loss: 0.0994\n",
            "Dataset: PubMed, Model: GAT, Attack: DPGBA, Defense: None - ASR: 100.00%, Clean Accuracy: 87.55%\n",
            "Dataset: PubMed, Model: GAT, Attack: DPGBA, Defense: DOMINANT - ASR: 85.00%, Clean Accuracy: 77.99%\n",
            "Dataset: PubMed, Model: GAT, Attack: DPGBA, Defense: Prune - ASR: 92.50%, Clean Accuracy: 84.00%\n",
            "Dataset: PubMed, Model: GAT, Attack: DPGBA, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 44.36%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: CiteSeer, Model: GCN, Baseline Accuracy: 73.68%\n",
            "Epoch 0, Reconstruction Loss: 0.0086\n",
            "Epoch 10, Reconstruction Loss: 0.0083\n",
            "Epoch 20, Reconstruction Loss: 0.0082\n",
            "Epoch 30, Reconstruction Loss: 0.0081\n",
            "Epoch 40, Reconstruction Loss: 0.0081\n",
            "Epoch 50, Reconstruction Loss: 0.0080\n",
            "Epoch 60, Reconstruction Loss: 0.0079\n",
            "Epoch 70, Reconstruction Loss: 0.0078\n",
            "Epoch 80, Reconstruction Loss: 0.0078\n",
            "Epoch 90, Reconstruction Loss: 0.0077\n",
            "Epoch 0, Loss: 0.1269\n",
            "Epoch 10, Loss: 0.1234\n",
            "Epoch 20, Loss: 0.1152\n",
            "Epoch 30, Loss: 0.1108\n",
            "Epoch 40, Loss: 0.1056\n",
            "Epoch 50, Loss: 0.1067\n",
            "Epoch 60, Loss: 0.1042\n",
            "Epoch 70, Loss: 0.1023\n",
            "Epoch 80, Loss: 0.0964\n",
            "Epoch 90, Loss: 0.0952\n",
            "Dataset: CiteSeer, Model: GCN, Attack: SBA-Samp, Defense: None - ASR: 90.00%, Clean Accuracy: 73.83%\n",
            "Dataset: CiteSeer, Model: GCN, Attack: SBA-Samp, Defense: DOMINANT - ASR: 70.00%, Clean Accuracy: 65.26%\n",
            "Dataset: CiteSeer, Model: GCN, Attack: SBA-Samp, Defense: Prune - ASR: 73.33%, Clean Accuracy: 69.77%\n",
            "Dataset: CiteSeer, Model: GCN, Attack: SBA-Samp, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 36.84%\n",
            "Epoch 0, Loss: 0.0905\n",
            "Epoch 10, Loss: 0.0884\n",
            "Epoch 20, Loss: 0.0847\n",
            "Epoch 30, Loss: 0.0868\n",
            "Epoch 40, Loss: 0.0847\n",
            "Epoch 50, Loss: 0.0808\n",
            "Epoch 60, Loss: 0.0793\n",
            "Epoch 70, Loss: 0.0759\n",
            "Epoch 80, Loss: 0.0774\n",
            "Epoch 90, Loss: 0.0773\n",
            "Dataset: CiteSeer, Model: GCN, Attack: SBA-Gen, Defense: None - ASR: 90.00%, Clean Accuracy: 73.53%\n",
            "Dataset: CiteSeer, Model: GCN, Attack: SBA-Gen, Defense: DOMINANT - ASR: 66.67%, Clean Accuracy: 65.71%\n",
            "Dataset: CiteSeer, Model: GCN, Attack: SBA-Gen, Defense: Prune - ASR: 73.33%, Clean Accuracy: 69.02%\n",
            "Dataset: CiteSeer, Model: GCN, Attack: SBA-Gen, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 37.59%\n",
            "Epoch 0, Loss: 0.0810\n",
            "Epoch 10, Loss: 0.0749\n",
            "Epoch 20, Loss: 0.0736\n",
            "Epoch 30, Loss: 0.0727\n",
            "Epoch 40, Loss: 0.0740\n",
            "Epoch 50, Loss: 0.0705\n",
            "Epoch 60, Loss: 0.0693\n",
            "Epoch 70, Loss: 0.0681\n",
            "Epoch 80, Loss: 0.0672\n",
            "Epoch 90, Loss: 0.0638\n",
            "Dataset: CiteSeer, Model: GCN, Attack: GTA, Defense: None - ASR: 90.00%, Clean Accuracy: 72.93%\n",
            "Dataset: CiteSeer, Model: GCN, Attack: GTA, Defense: DOMINANT - ASR: 90.00%, Clean Accuracy: 65.41%\n",
            "Dataset: CiteSeer, Model: GCN, Attack: GTA, Defense: Prune - ASR: 80.00%, Clean Accuracy: 67.52%\n",
            "Dataset: CiteSeer, Model: GCN, Attack: GTA, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 36.84%\n",
            "Epoch 0, Loss: 1.1483\n",
            "Epoch 10, Loss: 0.3637\n",
            "Epoch 20, Loss: 0.1364\n",
            "Epoch 30, Loss: 0.1014\n",
            "Epoch 40, Loss: 0.0861\n",
            "Epoch 50, Loss: 0.0757\n",
            "Epoch 60, Loss: 0.0755\n",
            "Epoch 70, Loss: 0.0732\n",
            "Epoch 80, Loss: 0.0705\n",
            "Epoch 90, Loss: 0.0690\n",
            "Dataset: CiteSeer, Model: GCN, Attack: UGBA, Defense: None - ASR: 90.00%, Clean Accuracy: 73.38%\n",
            "Dataset: CiteSeer, Model: GCN, Attack: UGBA, Defense: DOMINANT - ASR: 60.00%, Clean Accuracy: 65.41%\n",
            "Dataset: CiteSeer, Model: GCN, Attack: UGBA, Defense: Prune - ASR: 73.33%, Clean Accuracy: 68.12%\n",
            "Dataset: CiteSeer, Model: GCN, Attack: UGBA, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 38.05%\n",
            "Epoch 0, Loss: 0.0666\n",
            "Epoch 10, Loss: 0.0630\n",
            "Epoch 20, Loss: 0.0631\n",
            "Epoch 30, Loss: 0.0615\n",
            "Epoch 40, Loss: 0.0653\n",
            "Epoch 50, Loss: 0.0627\n",
            "Epoch 60, Loss: 0.0608\n",
            "Epoch 70, Loss: 0.0610\n",
            "Epoch 80, Loss: 0.0612\n",
            "Epoch 90, Loss: 0.0611\n",
            "Dataset: CiteSeer, Model: GCN, Attack: DPGBA, Defense: None - ASR: 90.00%, Clean Accuracy: 72.78%\n",
            "Dataset: CiteSeer, Model: GCN, Attack: DPGBA, Defense: DOMINANT - ASR: 90.00%, Clean Accuracy: 65.11%\n",
            "Dataset: CiteSeer, Model: GCN, Attack: DPGBA, Defense: Prune - ASR: 73.33%, Clean Accuracy: 68.12%\n",
            "Dataset: CiteSeer, Model: GCN, Attack: DPGBA, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 37.89%\n",
            "Dataset: CiteSeer, Model: GraphSage, Baseline Accuracy: 76.09%\n",
            "Epoch 0, Reconstruction Loss: 0.0086\n",
            "Epoch 10, Reconstruction Loss: 0.0083\n",
            "Epoch 20, Reconstruction Loss: 0.0082\n",
            "Epoch 30, Reconstruction Loss: 0.0081\n",
            "Epoch 40, Reconstruction Loss: 0.0081\n",
            "Epoch 50, Reconstruction Loss: 0.0080\n",
            "Epoch 60, Reconstruction Loss: 0.0079\n",
            "Epoch 70, Reconstruction Loss: 0.0078\n",
            "Epoch 80, Reconstruction Loss: 0.0078\n",
            "Epoch 90, Reconstruction Loss: 0.0077\n",
            "Epoch 0, Loss: 0.0098\n",
            "Epoch 10, Loss: 0.0091\n",
            "Epoch 20, Loss: 0.0078\n",
            "Epoch 30, Loss: 0.0061\n",
            "Epoch 40, Loss: 0.0066\n",
            "Epoch 50, Loss: 0.0064\n",
            "Epoch 60, Loss: 0.0070\n",
            "Epoch 70, Loss: 0.0054\n",
            "Epoch 80, Loss: 0.0060\n",
            "Epoch 90, Loss: 0.0043\n",
            "Dataset: CiteSeer, Model: GraphSage, Attack: SBA-Samp, Defense: None - ASR: 90.00%, Clean Accuracy: 76.09%\n",
            "Dataset: CiteSeer, Model: GraphSage, Attack: SBA-Samp, Defense: DOMINANT - ASR: 70.00%, Clean Accuracy: 67.82%\n",
            "Dataset: CiteSeer, Model: GraphSage, Attack: SBA-Samp, Defense: Prune - ASR: 86.67%, Clean Accuracy: 70.08%\n",
            "Dataset: CiteSeer, Model: GraphSage, Attack: SBA-Samp, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 36.99%\n",
            "Epoch 0, Loss: 0.0049\n",
            "Epoch 10, Loss: 0.0045\n",
            "Epoch 20, Loss: 0.0044\n",
            "Epoch 30, Loss: 0.0052\n",
            "Epoch 40, Loss: 0.0045\n",
            "Epoch 50, Loss: 0.0034\n",
            "Epoch 60, Loss: 0.0033\n",
            "Epoch 70, Loss: 0.0035\n",
            "Epoch 80, Loss: 0.0035\n",
            "Epoch 90, Loss: 0.0034\n",
            "Dataset: CiteSeer, Model: GraphSage, Attack: SBA-Gen, Defense: None - ASR: 90.00%, Clean Accuracy: 76.24%\n",
            "Dataset: CiteSeer, Model: GraphSage, Attack: SBA-Gen, Defense: DOMINANT - ASR: 73.33%, Clean Accuracy: 67.82%\n",
            "Dataset: CiteSeer, Model: GraphSage, Attack: SBA-Gen, Defense: Prune - ASR: 86.67%, Clean Accuracy: 70.23%\n",
            "Dataset: CiteSeer, Model: GraphSage, Attack: SBA-Gen, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 38.05%\n",
            "Epoch 0, Loss: 0.0075\n",
            "Epoch 10, Loss: 0.0036\n",
            "Epoch 20, Loss: 0.0031\n",
            "Epoch 30, Loss: 0.0031\n",
            "Epoch 40, Loss: 0.0029\n",
            "Epoch 50, Loss: 0.0030\n",
            "Epoch 60, Loss: 0.0026\n",
            "Epoch 70, Loss: 0.0028\n",
            "Epoch 80, Loss: 0.0021\n",
            "Epoch 90, Loss: 0.0024\n",
            "Dataset: CiteSeer, Model: GraphSage, Attack: GTA, Defense: None - ASR: 93.33%, Clean Accuracy: 76.09%\n",
            "Dataset: CiteSeer, Model: GraphSage, Attack: GTA, Defense: DOMINANT - ASR: 93.33%, Clean Accuracy: 67.22%\n",
            "Dataset: CiteSeer, Model: GraphSage, Attack: GTA, Defense: Prune - ASR: 93.33%, Clean Accuracy: 69.62%\n",
            "Dataset: CiteSeer, Model: GraphSage, Attack: GTA, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 37.14%\n",
            "Epoch 0, Loss: 0.2694\n",
            "Epoch 10, Loss: 0.1868\n",
            "Epoch 20, Loss: 0.0859\n",
            "Epoch 30, Loss: 0.0295\n",
            "Epoch 40, Loss: 0.0432\n",
            "Epoch 50, Loss: 0.0113\n",
            "Epoch 60, Loss: 0.0310\n",
            "Epoch 70, Loss: 0.0077\n",
            "Epoch 80, Loss: 0.0051\n",
            "Epoch 90, Loss: 0.0057\n",
            "Dataset: CiteSeer, Model: GraphSage, Attack: UGBA, Defense: None - ASR: 90.00%, Clean Accuracy: 76.24%\n",
            "Dataset: CiteSeer, Model: GraphSage, Attack: UGBA, Defense: DOMINANT - ASR: 60.00%, Clean Accuracy: 68.57%\n",
            "Dataset: CiteSeer, Model: GraphSage, Attack: UGBA, Defense: Prune - ASR: 86.67%, Clean Accuracy: 68.42%\n",
            "Dataset: CiteSeer, Model: GraphSage, Attack: UGBA, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 36.84%\n",
            "Epoch 0, Loss: 0.0042\n",
            "Epoch 10, Loss: 0.0037\n",
            "Epoch 20, Loss: 0.0033\n",
            "Epoch 30, Loss: 0.0030\n",
            "Epoch 40, Loss: 0.0026\n",
            "Epoch 50, Loss: 0.0025\n",
            "Epoch 60, Loss: 0.0029\n",
            "Epoch 70, Loss: 0.0022\n",
            "Epoch 80, Loss: 0.0033\n",
            "Epoch 90, Loss: 0.0023\n",
            "Dataset: CiteSeer, Model: GraphSage, Attack: DPGBA, Defense: None - ASR: 93.33%, Clean Accuracy: 76.39%\n",
            "Dataset: CiteSeer, Model: GraphSage, Attack: DPGBA, Defense: DOMINANT - ASR: 93.33%, Clean Accuracy: 68.57%\n",
            "Dataset: CiteSeer, Model: GraphSage, Attack: DPGBA, Defense: Prune - ASR: 90.00%, Clean Accuracy: 67.82%\n",
            "Dataset: CiteSeer, Model: GraphSage, Attack: DPGBA, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 36.24%\n",
            "Dataset: CiteSeer, Model: GAT, Baseline Accuracy: 75.34%\n",
            "Epoch 0, Reconstruction Loss: 0.0086\n",
            "Epoch 10, Reconstruction Loss: 0.0083\n",
            "Epoch 20, Reconstruction Loss: 0.0082\n",
            "Epoch 30, Reconstruction Loss: 0.0081\n",
            "Epoch 40, Reconstruction Loss: 0.0081\n",
            "Epoch 50, Reconstruction Loss: 0.0080\n",
            "Epoch 60, Reconstruction Loss: 0.0079\n",
            "Epoch 70, Reconstruction Loss: 0.0078\n",
            "Epoch 80, Reconstruction Loss: 0.0078\n",
            "Epoch 90, Reconstruction Loss: 0.0077\n",
            "Epoch 0, Loss: 0.0522\n",
            "Epoch 10, Loss: 0.0440\n",
            "Epoch 20, Loss: 0.0423\n",
            "Epoch 30, Loss: 0.0449\n",
            "Epoch 40, Loss: 0.0424\n",
            "Epoch 50, Loss: 0.0395\n",
            "Epoch 60, Loss: 0.0398\n",
            "Epoch 70, Loss: 0.0409\n",
            "Epoch 80, Loss: 0.0404\n",
            "Epoch 90, Loss: 0.0390\n",
            "Dataset: CiteSeer, Model: GAT, Attack: SBA-Samp, Defense: None - ASR: 93.33%, Clean Accuracy: 74.59%\n",
            "Dataset: CiteSeer, Model: GAT, Attack: SBA-Samp, Defense: DOMINANT - ASR: 73.33%, Clean Accuracy: 66.32%\n",
            "Dataset: CiteSeer, Model: GAT, Attack: SBA-Samp, Defense: Prune - ASR: 80.00%, Clean Accuracy: 71.13%\n",
            "Dataset: CiteSeer, Model: GAT, Attack: SBA-Samp, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 36.99%\n",
            "Epoch 0, Loss: 0.0457\n",
            "Epoch 10, Loss: 0.0393\n",
            "Epoch 20, Loss: 0.0374\n",
            "Epoch 30, Loss: 0.0393\n",
            "Epoch 40, Loss: 0.0380\n",
            "Epoch 50, Loss: 0.0373\n",
            "Epoch 60, Loss: 0.0364\n",
            "Epoch 70, Loss: 0.0359\n",
            "Epoch 80, Loss: 0.0371\n",
            "Epoch 90, Loss: 0.0351\n",
            "Dataset: CiteSeer, Model: GAT, Attack: SBA-Gen, Defense: None - ASR: 93.33%, Clean Accuracy: 75.19%\n",
            "Dataset: CiteSeer, Model: GAT, Attack: SBA-Gen, Defense: DOMINANT - ASR: 73.33%, Clean Accuracy: 68.42%\n",
            "Dataset: CiteSeer, Model: GAT, Attack: SBA-Gen, Defense: Prune - ASR: 80.00%, Clean Accuracy: 71.43%\n",
            "Dataset: CiteSeer, Model: GAT, Attack: SBA-Gen, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 38.35%\n",
            "Epoch 0, Loss: 0.0442\n",
            "Epoch 10, Loss: 0.0356\n",
            "Epoch 20, Loss: 0.0361\n",
            "Epoch 30, Loss: 0.0358\n",
            "Epoch 40, Loss: 0.0349\n",
            "Epoch 50, Loss: 0.0354\n",
            "Epoch 60, Loss: 0.0344\n",
            "Epoch 70, Loss: 0.0342\n",
            "Epoch 80, Loss: 0.0346\n",
            "Epoch 90, Loss: 0.0351\n",
            "Dataset: CiteSeer, Model: GAT, Attack: GTA, Defense: None - ASR: 93.33%, Clean Accuracy: 74.74%\n",
            "Dataset: CiteSeer, Model: GAT, Attack: GTA, Defense: DOMINANT - ASR: 96.67%, Clean Accuracy: 67.07%\n",
            "Dataset: CiteSeer, Model: GAT, Attack: GTA, Defense: Prune - ASR: 86.67%, Clean Accuracy: 72.03%\n",
            "Dataset: CiteSeer, Model: GAT, Attack: GTA, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 37.89%\n",
            "Epoch 0, Loss: 3.5551\n",
            "Epoch 10, Loss: 1.6941\n",
            "Epoch 20, Loss: 1.9362\n",
            "Epoch 30, Loss: 1.3287\n",
            "Epoch 40, Loss: 1.0784\n",
            "Epoch 50, Loss: 3.3299\n",
            "Epoch 60, Loss: 1.9682\n",
            "Epoch 70, Loss: 2.8384\n",
            "Epoch 80, Loss: 1.4989\n",
            "Epoch 90, Loss: 1.4508\n",
            "Dataset: CiteSeer, Model: GAT, Attack: UGBA, Defense: None - ASR: 93.33%, Clean Accuracy: 74.74%\n",
            "Dataset: CiteSeer, Model: GAT, Attack: UGBA, Defense: DOMINANT - ASR: 73.33%, Clean Accuracy: 64.36%\n",
            "Dataset: CiteSeer, Model: GAT, Attack: UGBA, Defense: Prune - ASR: 70.00%, Clean Accuracy: 73.38%\n",
            "Dataset: CiteSeer, Model: GAT, Attack: UGBA, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 39.10%\n",
            "Epoch 0, Loss: 0.2798\n",
            "Epoch 10, Loss: 0.1980\n",
            "Epoch 20, Loss: 0.2064\n",
            "Epoch 30, Loss: 0.1813\n",
            "Epoch 40, Loss: 0.1831\n",
            "Epoch 50, Loss: 0.1710\n",
            "Epoch 60, Loss: 0.1362\n",
            "Epoch 70, Loss: 0.1334\n",
            "Epoch 80, Loss: 0.1498\n",
            "Epoch 90, Loss: 0.1665\n",
            "Dataset: CiteSeer, Model: GAT, Attack: DPGBA, Defense: None - ASR: 93.33%, Clean Accuracy: 74.74%\n",
            "Dataset: CiteSeer, Model: GAT, Attack: DPGBA, Defense: DOMINANT - ASR: 93.33%, Clean Accuracy: 66.77%\n",
            "Dataset: CiteSeer, Model: GAT, Attack: DPGBA, Defense: Prune - ASR: 83.33%, Clean Accuracy: 73.83%\n",
            "Dataset: CiteSeer, Model: GAT, Attack: DPGBA, Defense: Prune+LD - ASR: 0.00%, Clean Accuracy: 39.25%\n",
            "\n",
            "Summary of Results:\n",
            "      Dataset Model    Attack                         Defense        ASR  \\\n",
            "0        Cora   GCN      None                            None        N/A   \n",
            "1        Cora   GCN  SBA-Samp                            None      100.0   \n",
            "2        Cora   GCN  SBA-Samp  Dominant Set Outlier Detection      100.0   \n",
            "3        Cora   GCN  SBA-Samp                           Prune       90.0   \n",
            "4        Cora   GCN  SBA-Samp                      Prune + LD        0.0   \n",
            "..        ...   ...       ...                             ...        ...   \n",
            "184  CiteSeer   GAT      UGBA                      Prune + LD        0.0   \n",
            "185  CiteSeer   GAT     DPGBA                            None  93.333333   \n",
            "186  CiteSeer   GAT     DPGBA  Dominant Set Outlier Detection  93.333333   \n",
            "187  CiteSeer   GAT     DPGBA                           Prune  83.333333   \n",
            "188  CiteSeer   GAT     DPGBA                      Prune + LD        0.0   \n",
            "\n",
            "     Clean Accuracy  \n",
            "0         87.800370  \n",
            "1         87.800370  \n",
            "2         78.373383  \n",
            "3         79.852126  \n",
            "4         44.362292  \n",
            "..              ...  \n",
            "184       39.097744  \n",
            "185       74.736842  \n",
            "186       66.766917  \n",
            "187       73.834586  \n",
            "188       39.248120  \n",
            "\n",
            "[189 rows x 6 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Install necessary packages\n",
        "!pip install torch-geometric\n",
        "!pip install matplotlib\n",
        "!pip install scikit-learn\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
        "from torch_geometric.datasets import Planetoid, Flickr\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "import networkx as nx\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import pandas as pd\n",
        "from torch_geometric.utils import to_networkx\n",
        "from torch_geometric.nn.models.autoencoder import GAE\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "\n",
        "set_seed()\n",
        "\n",
        "# Check if GPU is available and set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# Load datasets\n",
        "def load_dataset(dataset_name):\n",
        "    if dataset_name in [\"Cora\", \"PubMed\", \"CiteSeer\"]:\n",
        "        dataset = Planetoid(root=f\"./data/{dataset_name}\", name=dataset_name)\n",
        "    elif dataset_name == \"Flickr\":\n",
        "        dataset = Flickr(root=\"./data/Flickr\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Split dataset into train/validation/test\n",
        "# Updated to randomly mask out 20% of nodes, use 10% for labeled nodes, and 10% for validation\n",
        "def split_dataset(data, test_size=0.2, val_size=0.1):\n",
        "    num_nodes = data.num_nodes\n",
        "    indices = np.arange(num_nodes)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    num_test = int(test_size * num_nodes)\n",
        "    num_val = int(val_size * num_nodes)\n",
        "    num_train = num_nodes - num_test - num_val\n",
        "\n",
        "    train_mask = torch.zeros(num_nodes, dtype=torch.bool).to(device)\n",
        "    val_mask = torch.zeros(num_nodes, dtype=torch.bool).to(device)\n",
        "    test_mask = torch.zeros(num_nodes, dtype=torch.bool).to(device)\n",
        "\n",
        "    train_mask[indices[:num_train]] = True\n",
        "    val_mask[indices[num_train:num_train + num_val]] = True\n",
        "    test_mask[indices[num_train + num_val:]] = True\n",
        "\n",
        "    data.train_mask = train_mask\n",
        "    data.val_mask = val_mask\n",
        "    data.test_mask = test_mask\n",
        "\n",
        "    # Mask out 20% nodes for attack performance evaluation (half target, half clean test)\n",
        "    num_target = int(0.1 * num_nodes)  # Half of 20%\n",
        "    target_mask = torch.zeros(num_nodes, dtype=torch.bool).to(device)\n",
        "    clean_test_mask = torch.zeros(num_nodes, dtype=torch.bool).to(device)\n",
        "    target_mask[indices[num_train + num_val:num_train + num_val + num_target]] = True\n",
        "    clean_test_mask[indices[num_train + num_val + num_target:]] = True\n",
        "\n",
        "    data.target_mask = target_mask\n",
        "    data.clean_test_mask = clean_test_mask\n",
        "\n",
        "    return data\n",
        "\n",
        "# Define GNN Model with multiple architectures (GCN, GraphSAGE, GAT)\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, model_type='GCN'):\n",
        "        super(GNN, self).__init__()\n",
        "        if model_type == 'GCN':\n",
        "            self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "            self.conv2 = GCNConv(hidden_dim, output_dim)\n",
        "        elif model_type == 'GraphSage':\n",
        "            self.conv1 = SAGEConv(input_dim, hidden_dim)\n",
        "            self.conv2 = SAGEConv(hidden_dim, output_dim)\n",
        "        elif model_type == 'GAT':\n",
        "            self.conv1 = GATConv(input_dim, hidden_dim, heads=8, concat=True)\n",
        "            self.conv2 = GATConv(hidden_dim * 8, output_dim, heads=1, concat=False)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Select nodes to poison based on high-centrality (degree centrality) for a stronger impact\n",
        "def select_high_centrality_nodes(data, num_nodes_to_select):\n",
        "    graph = nx.Graph()\n",
        "    edge_index = data.edge_index.cpu().numpy()\n",
        "    graph.add_edges_from(edge_index.T)\n",
        "    centrality = nx.degree_centrality(graph)\n",
        "    sorted_nodes = sorted(centrality, key=centrality.get, reverse=True)\n",
        "    return torch.tensor(sorted_nodes[:num_nodes_to_select], dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "class TriggerGenerator(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(TriggerGenerator, self).__init__()\n",
        "        self.mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_dim, hidden_dim),  # Reduce dimensions\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hidden_dim, input_dim)  # Restore to input_dim\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "class GAE(torch.nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(GAE, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def encode(self, x, edge_index):\n",
        "        return self.encoder(x, edge_index)\n",
        "\n",
        "    def decode(self, z, edge_index):\n",
        "        return self.decoder(z, edge_index)\n",
        "\n",
        "class OODDetector(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(OODDetector, self).__init__()\n",
        "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
        "        self.decoder = Decoder(input_dim, hidden_dim, latent_dim)\n",
        "        self.gae = GAE(self.encoder, self.decoder)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        z = self.gae.encode(x, edge_index)\n",
        "        return z\n",
        "\n",
        "    def reconstruct(self, z, edge_index):\n",
        "        return self.decoder(z, edge_index)\n",
        "\n",
        "    def reconstruction_loss(self, x, edge_index):\n",
        "        z = self.gae.encode(x, edge_index)\n",
        "        reconstructed = self.reconstruct(z, edge_index)\n",
        "        return F.mse_loss(reconstructed, x)\n",
        "\n",
        "    def detect_ood(self, x, edge_index, threshold):\n",
        "        loss = self.reconstruction_loss(x, edge_index)\n",
        "        return loss > threshold\n",
        "\n",
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        z = F.relu(self.conv1(x, edge_index))\n",
        "        z = self.conv2(z, edge_index)\n",
        "        return z\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.conv1 = GCNConv(latent_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, input_dim)\n",
        "\n",
        "    def forward(self, z, edge_index):\n",
        "        x = F.relu(self.conv1(z, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "def train_ood_detector(ood_detector, data, optimizer, epochs=50):\n",
        "    ood_detector.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()  # Clear the gradients\n",
        "\n",
        "        # Forward pass\n",
        "        z = ood_detector(data.x, data.edge_index)\n",
        "\n",
        "        # Reconstruct data using the latent embedding\n",
        "        reconstructed_x = ood_detector.reconstruct(z, data.edge_index)\n",
        "\n",
        "        # Use only the training mask to compute reconstruction loss\n",
        "        loss = F.mse_loss(reconstructed_x[data.train_mask], data.x[data.train_mask])\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        # Print loss every 10 epochs\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Reconstruction Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "def train_with_poisoned_data(model, data, optimizer, poisoned_nodes, trigger_gen, attack, ood_detector=None, alpha=0.7, early_stopping=False):\n",
        "    # Apply trigger injection\n",
        "    data_poisoned = inject_trigger(data, poisoned_nodes, attack, trigger_gen, ood_detector, alpha)\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    for epoch in range(100):\n",
        "        optimizer.zero_grad()  # Clear the gradients\n",
        "\n",
        "        # Forward pass\n",
        "        out = model(data_poisoned.x, data_poisoned.edge_index)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = F.cross_entropy(out[data_poisoned.train_mask], data_poisoned.y[data_poisoned.train_mask])\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()  # No retain_graph=True unless explicitly required\n",
        "        optimizer.step()\n",
        "\n",
        "        # Optional: Print loss during training for insight\n",
        "        if early_stopping and epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    return model, data_poisoned\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "class GCNEncoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GCNEncoder, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
        "        self.conv2 = GCNConv(2 * out_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "def select_diverse_nodes(data, num_nodes_to_select, num_clusters=None):\n",
        "    \"\"\"\n",
        "    Select nodes using a clustering-based approach to ensure diversity, along with high-degree nodes.\n",
        "\n",
        "    Parameters:\n",
        "    - data: PyG data object representing the graph.\n",
        "    - num_nodes_to_select: Number of nodes to select for poisoning.\n",
        "    - num_clusters: Number of clusters to form for diversity. Defaults to number of classes if not provided.\n",
        "\n",
        "    Returns:\n",
        "    - Tensor containing indices of selected nodes.\n",
        "    \"\"\"\n",
        "    # Set the number of clusters equal to the number of classes in the dataset if not provided\n",
        "    if num_clusters is None:\n",
        "        num_clusters = len(torch.unique(data.y))\n",
        "\n",
        "    # Use GCN encoder to get node embeddings that capture both attribute and structural information\n",
        "    encoder = GCNEncoder(data.num_features, out_channels=16)  # Assuming out_channels = 16\n",
        "    encoder.eval()\n",
        "    with torch.no_grad():\n",
        "        embeddings = encoder(data.x, data.edge_index).cpu().numpy()\n",
        "\n",
        "    # Perform K-means clustering to find representative nodes\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(embeddings)\n",
        "    labels = kmeans.labels_\n",
        "    cluster_centers = kmeans.cluster_centers_\n",
        "\n",
        "    # Select nodes closest to the cluster centers\n",
        "    selected_nodes = []\n",
        "    for i in range(num_clusters):\n",
        "        cluster_indices = np.where(labels == i)[0]\n",
        "        center = cluster_centers[i]\n",
        "        distances = np.linalg.norm(embeddings[cluster_indices] - center, axis=1)\n",
        "        closest_node = cluster_indices[np.argmin(distances)]\n",
        "        selected_nodes.append(closest_node)\n",
        "\n",
        "    # Calculate node degrees\n",
        "    degree = torch.bincount(data.edge_index[0])  # Calculate node degrees\n",
        "    # Select high-degree nodes\n",
        "    high_degree_nodes = torch.topk(degree, len(selected_nodes) // 2).indices\n",
        "\n",
        "    # Convert the graph to NetworkX to calculate centrality measures\n",
        "    G = to_networkx(data, to_undirected=True)\n",
        "    betweenness_centrality = nx.betweenness_centrality(G)\n",
        "    central_nodes = sorted(betweenness_centrality, key=betweenness_centrality.get, reverse=True)\n",
        "    central_nodes_tensor = torch.tensor(central_nodes[:len(selected_nodes) // 2], dtype=torch.long)\n",
        "\n",
        "    # Combine diverse nodes, high-degree nodes, and central nodes\n",
        "    combined_nodes = torch.cat([torch.tensor(selected_nodes), high_degree_nodes, central_nodes_tensor])\n",
        "    # Get unique nodes and limit to num_nodes_to_select\n",
        "    unique_nodes = torch.unique(combined_nodes)[:num_nodes_to_select]\n",
        "\n",
        "    return torch.tensor(selected_nodes[:num_nodes_to_select], dtype=torch.long).to(data.x.device)\n",
        "\n",
        "def inject_trigger(data, poisoned_nodes, attack_type, trigger_gen=None, ood_detector=None, alpha=0.7, trigger_size=5, trigger_density=0.5, input_dim=None):\n",
        "    # Clone data to avoid overwriting the original graph\n",
        "    data_poisoned = data.clone()\n",
        "    device = data_poisoned.x.device\n",
        "\n",
        "    if len(poisoned_nodes) == 0:\n",
        "        raise ValueError(\"No poisoned nodes selected. Ensure 'poisoned_nodes' is populated and non-empty.\")\n",
        "\n",
        "    # Adjust trigger_size if it exceeds the number of poisoned nodes\n",
        "    trigger_size = min(trigger_size, len(poisoned_nodes))\n",
        "\n",
        "    if attack_type == 'SBA-Samp':\n",
        "        # Subgraph-Based Attack - Random Sampling\n",
        "        connected_nodes = [data.edge_index[0][data.edge_index[1] == node] for node in poisoned_nodes[:trigger_size]]\n",
        "        avg_features = torch.stack([\n",
        "            data.x[nodes].mean(dim=0) if len(nodes) > 0 else data.x.mean(dim=0) for nodes in connected_nodes\n",
        "        ])\n",
        "        natural_features = avg_features + torch.randn_like(avg_features) * 0.02  # Small randomness\n",
        "\n",
        "        # Generate subgraph with realistic density\n",
        "        G = nx.erdos_renyi_graph(trigger_size, trigger_density)\n",
        "        trigger_edge_index = torch.tensor(list(G.edges)).t().contiguous()\n",
        "\n",
        "        # Connect poisoned nodes to the subgraph\n",
        "        poisoned_edges = torch.stack([\n",
        "            poisoned_nodes[:trigger_size],\n",
        "            torch.randint(0, data.num_nodes, (trigger_size,), device=device)\n",
        "        ])\n",
        "\n",
        "        # Update graph structure and features\n",
        "        data_poisoned.edge_index = torch.cat([data.edge_index, trigger_edge_index.to(device), poisoned_edges.to(device)], dim=1)\n",
        "        data_poisoned.x[poisoned_nodes[:trigger_size]] = natural_features[:trigger_size]\n",
        "\n",
        "    elif attack_type == 'SBA-Gen':\n",
        "        # Subgraph-Based Attack - Gaussian\n",
        "        connected_nodes = [data.edge_index[0][data.edge_index[1] == node] for node in poisoned_nodes[:trigger_size]]\n",
        "\n",
        "        # Calculate mean and standard deviation of features\n",
        "        feature_mean = data.x.mean(dim=0)\n",
        "        feature_std = data.x.std(dim=0)\n",
        "\n",
        "        # Generate natural features with Gaussian noise\n",
        "        avg_features = torch.stack([\n",
        "            data.x[nodes].mean(dim=0) if len(nodes) > 0 else feature_mean for nodes in connected_nodes\n",
        "        ])\n",
        "        natural_features = avg_features + torch.normal(mean=0.0, std=0.03, size=avg_features.shape).to(data.x.device)\n",
        "\n",
        "        # Generate subgraph edges based on Gaussian similarity\n",
        "        trigger_edge_index = []\n",
        "        for i in range(trigger_size):\n",
        "            for j in range(i + 1, trigger_size):\n",
        "                similarity = torch.exp(-torch.norm((natural_features[i] - natural_features[j]) / feature_std)**2)\n",
        "                if similarity > torch.rand(1).item():  # Random threshold\n",
        "                    trigger_edge_index.append([i, j])\n",
        "\n",
        "        # Convert edges to PyTorch tensor\n",
        "        trigger_edge_index = torch.tensor(trigger_edge_index, dtype=torch.long).t().contiguous()\n",
        "\n",
        "        # Check if trigger_edge_index is non-empty\n",
        "        if trigger_edge_index.numel() > 0:  # Ensure there are edges before modifying\n",
        "            trigger_edge_index += poisoned_nodes[:trigger_size].unsqueeze(0)\n",
        "\n",
        "        # Connect generated subgraph to existing nodes\n",
        "        poisoned_edges = torch.stack([\n",
        "            poisoned_nodes[:trigger_size],\n",
        "            torch.randint(0, data.num_nodes, (trigger_size,), device=device)\n",
        "        ])\n",
        "\n",
        "        # Update graph structure and features\n",
        "        data_poisoned.edge_index = torch.cat([data.edge_index, trigger_edge_index.to(device), poisoned_edges.to(device)], dim=1)\n",
        "        data_poisoned.x[poisoned_nodes[:trigger_size]] = natural_features[:trigger_size]\n",
        "\n",
        "\n",
        "    elif attack_type == 'DPGBA':\n",
        "        connected_nodes = [data.edge_index[0][data.edge_index[1] == node] for node in poisoned_nodes]\n",
        "        avg_features = torch.stack([\n",
        "            data.x[nodes].mean(dim=0) if len(nodes) > 0 else data.x.mean(dim=0) for nodes in connected_nodes\n",
        "        ]).to(device)\n",
        "\n",
        "        # Generate trigger features using the trigger generator\n",
        "        if trigger_gen is None:\n",
        "            raise ValueError(\"Trigger generator is required for the DPGBA attack.\")\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient tracking for feature generation\n",
        "            trigger_features = trigger_gen(avg_features)\n",
        "\n",
        "        # Validate dimensions\n",
        "        if trigger_features.shape[1] != data.x.shape[1]:\n",
        "            raise ValueError(f\"Trigger feature dimension mismatch: {trigger_features.shape[1]} vs {data.x.shape[1]}\")\n",
        "\n",
        "        # Blend features\n",
        "        node_alphas = torch.rand(len(poisoned_nodes)).to(device) * 0.3 + 0.5\n",
        "        distribution_preserved_features = (\n",
        "            node_alphas.unsqueeze(1) * data.x[poisoned_nodes]\n",
        "            + (1 - node_alphas.unsqueeze(1)) * trigger_features\n",
        "        )\n",
        "\n",
        "        # Update poisoned nodes\n",
        "        data_poisoned.x[poisoned_nodes] = distribution_preserved_features\n",
        "\n",
        "\n",
        "    elif attack_type == 'GTA':\n",
        "        # Graph Trojan Attack\n",
        "        connected_nodes = [data.edge_index[0][data.edge_index[1] == node] for node in poisoned_nodes]\n",
        "        avg_features = torch.stack([\n",
        "            data.x[nodes].mean(dim=0) if len(nodes) > 0 else data.x.mean(dim=0) for nodes in connected_nodes\n",
        "        ])\n",
        "        trigger_features = avg_features + torch.randn_like(avg_features) * 0.05\n",
        "        data_poisoned.x[poisoned_nodes] = trigger_features\n",
        "\n",
        "    elif attack_type == 'UGBA':\n",
        "        # Unnoticeable Graph Backdoor Attack\n",
        "        num_clusters = len(torch.unique(data_poisoned.y))\n",
        "        diverse_nodes = select_diverse_nodes(data_poisoned, len(poisoned_nodes))\n",
        "        connected_nodes = [data_poisoned.edge_index[0][data_poisoned.edge_index[1] == node] for node in diverse_nodes]\n",
        "\n",
        "        # Generate refined trigger features\n",
        "        avg_features = torch.stack([\n",
        "            data_poisoned.x[nodes].mean(dim=0) if len(nodes) > 0 else data_poisoned.x.mean(dim=0) for nodes in connected_nodes\n",
        "        ])\n",
        "        refined_trigger_features = avg_features + torch.normal(mean=2.0, std=0.5, size=avg_features.shape).to(data_poisoned.x.device)\n",
        "        data_poisoned.x[diverse_nodes] = refined_trigger_features\n",
        "\n",
        "        # Add edges between diverse nodes for structural blending\n",
        "        new_edges = []\n",
        "        for i in range(len(diverse_nodes)):\n",
        "            node = diverse_nodes[i]\n",
        "            neighbor = connected_nodes[i][0] if len(connected_nodes[i]) > 0 else diverse_nodes[(i + 1) % len(diverse_nodes)]\n",
        "            new_edges.append([node, neighbor])\n",
        "\n",
        "        # Update graph structure\n",
        "        new_edges = torch.tensor(new_edges, dtype=torch.long).t().contiguous().to(data_poisoned.edge_index.device)\n",
        "        data_poisoned.edge_index = torch.cat([data_poisoned.edge_index, new_edges], dim=1)\n",
        "\n",
        "    return data_poisoned\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import gc\n",
        "\n",
        "\n",
        "def dominant_set_clustering(data, threshold=0.7, use_pca=True, pca_components=10):\n",
        "    \"\"\"\n",
        "    Applies a simplified outlier detection framework using a combination of K-Means clustering and distance-based heuristics.\n",
        "\n",
        "    Parameters:\n",
        "    - data: PyG data object representing the graph.\n",
        "    - threshold: Quantile threshold for identifying outliers based on cluster distances.\n",
        "    - use_pca: Whether to use PCA for dimensionality reduction.\n",
        "    - pca_components: Number of PCA components to use if PCA is applied.\n",
        "\n",
        "    Returns:\n",
        "    - pruned_nodes: Set of nodes identified as outliers.\n",
        "    - data: Updated PyG data object with modified features and labels for outliers.\n",
        "    \"\"\"\n",
        "    # Step 1: Determine the number of clusters based on the number of classes\n",
        "    n_clusters = len(data.y.unique())  # Number of unique classes in the dataset\n",
        "\n",
        "    # Step 2: Dimensionality reduction using PCA (optional)\n",
        "    node_features = data.x.detach().cpu().numpy()\n",
        "    if use_pca and node_features.shape[1] > pca_components:\n",
        "        pca = PCA(n_components=pca_components)\n",
        "        node_features = pca.fit_transform(node_features)\n",
        "\n",
        "    # Step 3: K-Means Clustering to identify clusters and potential outliers\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(node_features)\n",
        "    cluster_labels = kmeans.labels_\n",
        "    cluster_centers = kmeans.cluster_centers_\n",
        "\n",
        "    # Calculate distances to cluster centers\n",
        "    distances = np.linalg.norm(node_features - cluster_centers[cluster_labels], axis=1)\n",
        "\n",
        "    # Identify outlier candidates based on distance threshold\n",
        "    distance_threshold = np.percentile(distances, 100 * threshold)\n",
        "    outlier_candidates = np.where(distances > distance_threshold)[0]\n",
        "\n",
        "    # Step 4: Update data to reflect removal of outlier influence\n",
        "    pruned_nodes = set(outlier_candidates)\n",
        "    if len(pruned_nodes) > 0:\n",
        "        outliers = torch.tensor(list(pruned_nodes), dtype=torch.long, device=data.x.device)\n",
        "\n",
        "        # Assign an invalid label (-1) to outlier nodes to discard them during training\n",
        "        data.y[outliers] = -1\n",
        "\n",
        "        # Replace the features of outliers with the average feature value to reduce their impact\n",
        "        data.x[outliers] = data.x.mean(dim=0).to(data.x.device)\n",
        "\n",
        "    return pruned_nodes, data\n",
        "\n",
        "def defense_prune_edges(data, quantile_threshold=0.9):\n",
        "    \"\"\"\n",
        "    Prunes edges based on adaptive cosine similarity between node features.\n",
        "\n",
        "    Parameters:\n",
        "    - data: PyG data object representing the graph.\n",
        "    - quantile_threshold: Quantile to determine pruning threshold (e.g., 0.9 means pruning edges in the top 10% dissimilar).\n",
        "\n",
        "    Returns:\n",
        "    - data: Updated PyG data object with pruned edges.\n",
        "    \"\"\"\n",
        "    features = data.x\n",
        "    norm_features = F.normalize(features, p=2, dim=1)  # Normalize features\n",
        "    edge_index = data.edge_index\n",
        "\n",
        "    # Calculate cosine similarity for each edge\n",
        "    src, dst = edge_index[0], edge_index[1]\n",
        "    cosine_similarities = torch.sum(norm_features[src] * norm_features[dst], dim=1)\n",
        "\n",
        "    # Adaptive threshold based on quantile of similarity distribution\n",
        "    similarity_threshold = torch.quantile(cosine_similarities, quantile_threshold).item()\n",
        "\n",
        "    # Keep edges with cosine similarity above the threshold\n",
        "    pruned_mask = cosine_similarities >= similarity_threshold\n",
        "    pruned_edges = edge_index[:, pruned_mask]\n",
        "\n",
        "    # Update edge index with pruned edges\n",
        "    data.edge_index = pruned_edges\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def defense_prune_and_discard_labels(data, quantile_threshold=0.2):\n",
        "    \"\"\"\n",
        "    Prunes edges based on adaptive cosine similarity and discards labels of nodes connected by pruned edges selectively.\n",
        "\n",
        "    Parameters:\n",
        "    - data: PyG data object representing the graph.\n",
        "    - quantile_threshold: Quantile threshold for cosine similarity pruning (e.g., 0.2 means pruning edges in the bottom 20%).\n",
        "\n",
        "    Returns:\n",
        "    - data: Updated PyG data object with pruned edges and selectively discarded labels.\n",
        "    \"\"\"\n",
        "    features = data.x\n",
        "    norm_features = F.normalize(features, p=2, dim=1)  # Normalize features using PyTorch\n",
        "    edge_index = data.edge_index\n",
        "\n",
        "    # Calculate cosine similarity for each edge\n",
        "    src, dst = edge_index[0], edge_index[1]\n",
        "    cosine_similarities = torch.sum(norm_features[src] * norm_features[dst], dim=1)\n",
        "\n",
        "    # Use quantile to determine adaptive threshold for pruning\n",
        "    adaptive_threshold = torch.quantile(cosine_similarities, quantile_threshold).item()\n",
        "\n",
        "    # Mask edges with similarity below the adaptive threshold\n",
        "    pruned_mask = cosine_similarities < adaptive_threshold\n",
        "    pruned_edges = edge_index[:, ~pruned_mask]  # Retain edges that are above the threshold\n",
        "\n",
        "    # Update edge index with pruned edges\n",
        "    data.edge_index = pruned_edges\n",
        "\n",
        "    # Selectively discard labels of nodes connected by many pruned edges\n",
        "    pruned_src, pruned_dst = edge_index[:, pruned_mask]\n",
        "    pruned_nodes_count = torch.bincount(torch.cat([pruned_src, pruned_dst]), minlength=data.num_nodes)\n",
        "\n",
        "    # Only discard labels if the node has a high count of pruned edges\n",
        "    threshold_count = int(torch.median(pruned_nodes_count).item())  # Use median count as a threshold\n",
        "    nodes_to_discard = torch.where(pruned_nodes_count > threshold_count)[0]\n",
        "\n",
        "    data.y[nodes_to_discard] = -1  # Use -1 to represent discarded labels\n",
        "\n",
        "    return data\n",
        "\n",
        "# Compute ASR and Clean Accuracy (using .detach() to avoid retaining computation graph)\n",
        "def compute_metrics(model, data, poisoned_nodes):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data.x, data.edge_index).detach()\n",
        "        _, pred = out.max(dim=1)\n",
        "        asr = (pred[poisoned_nodes] == data.y[poisoned_nodes]).sum().item() / len(poisoned_nodes) * 100\n",
        "        clean_acc = accuracy_score(data.y[data.test_mask].cpu(), pred[data.test_mask].cpu()) * 100\n",
        "    return asr, clean_acc\n",
        "\n",
        "\n",
        "\n",
        "# Visualization Function\n",
        "# Visualize PCA for Attacks\n",
        "# Added function to visualize PCA projections of node embeddings for different attacks\n",
        "def visualize_pca_for_attacks(attack_embeddings_dict):\n",
        "    pca = PCA(n_components=2)\n",
        "    plt.figure(figsize=(20, 10))\n",
        "\n",
        "    for i, (attack, attack_data) in enumerate(attack_embeddings_dict.items(), 1):\n",
        "        embeddings = attack_data['data'].detach().cpu().numpy()\n",
        "        poisoned_nodes = attack_data['poisoned_nodes'].detach().cpu().numpy()\n",
        "\n",
        "        # Apply PCA to the node embeddings\n",
        "        pca_result = pca.fit_transform(embeddings)\n",
        "\n",
        "        # Create masks for clean and poisoned nodes\n",
        "        clean_mask = np.ones(embeddings.shape[0], dtype=bool)\n",
        "        clean_mask[poisoned_nodes] = False\n",
        "\n",
        "        # Extract clean and poisoned node embeddings after PCA\n",
        "        clean_embeddings = pca_result[clean_mask]\n",
        "        poisoned_embeddings = pca_result[~clean_mask]\n",
        "\n",
        "        # Plotting clean and poisoned nodes\n",
        "        plt.subplot(2, 3, i)\n",
        "        plt.scatter(clean_embeddings[:, 0], clean_embeddings[:, 1], s=10, alpha=0.5, label='Clean Nodes', c='b')\n",
        "        plt.scatter(poisoned_embeddings[:, 0], poisoned_embeddings[:, 1], s=10, alpha=0.8, label='Poisoned Nodes', c='r')\n",
        "        plt.title(f'PCA Visualization for {attack}')\n",
        "        plt.xlabel('PCA Component 1')\n",
        "        plt.ylabel('PCA Component 2')\n",
        "        plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Run all attacks and apply defenses\n",
        "def run_all_attacks():\n",
        "    datasets = [\"Cora\", \"PubMed\", \"CiteSeer\"]\n",
        "    results_summary = []\n",
        "    attack_embeddings_dict = {}\n",
        "\n",
        "    for dataset_name in datasets:\n",
        "        try:\n",
        "            dataset = load_dataset(dataset_name)\n",
        "            data = dataset[0].to(device)\n",
        "            input_dim = data.num_features\n",
        "            output_dim = dataset.num_classes if isinstance(dataset.num_classes, int) else dataset.num_classes[0]\n",
        "            data = split_dataset(data)\n",
        "\n",
        "            # Dataset-specific poisoning budgets\n",
        "            dataset_budgets = {\n",
        "                'Cora': 10,\n",
        "                'PubMed': 40,\n",
        "                'CiteSeer': 30\n",
        "            }\n",
        "            poisoned_node_budget = dataset_budgets.get(dataset_name, 10)\n",
        "\n",
        "            # Define GNN models for experiments\n",
        "            model_types = ['GCN', 'GraphSage', 'GAT']\n",
        "\n",
        "            for model_type in model_types:\n",
        "                try:\n",
        "                    # Initialize model and optimizer\n",
        "                    model = GNN(input_dim=input_dim, hidden_dim=64, output_dim=output_dim, model_type=model_type).to(device)\n",
        "                    optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
        "\n",
        "                    # Train baseline model\n",
        "                    model.train()\n",
        "                    for epoch in range(200):\n",
        "                        optimizer.zero_grad()\n",
        "                        out = model(data.x, data.edge_index)\n",
        "                        loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                    # Evaluate baseline accuracy\n",
        "                    model.eval()\n",
        "                    with torch.no_grad():\n",
        "                        out = model(data.x, data.edge_index)\n",
        "                        predictions = out.argmax(dim=1)\n",
        "                        baseline_acc = (predictions[data.test_mask] == data.y[data.test_mask]).sum().item() / data.test_mask.sum().item()\n",
        "                        print(f\"Dataset: {dataset_name}, Model: {model_type}, Baseline Accuracy: {baseline_acc * 100:.2f}%\")\n",
        "\n",
        "                        results_summary.append({\n",
        "                            \"Dataset\": dataset_name,\n",
        "                            \"Model\": model_type,\n",
        "                            \"Attack\": \"None\",\n",
        "                            \"Defense\": \"None\",\n",
        "                            \"ASR\": \"N/A\",\n",
        "                            \"Clean Accuracy\": baseline_acc * 100\n",
        "                        })\n",
        "\n",
        "                    # Initialize Trigger Generator and OOD Detector\n",
        "                    trigger_gen = TriggerGenerator(input_dim=data.num_features, hidden_dim=64).to(device)\n",
        "                    ood_detector = OODDetector(input_dim=input_dim, hidden_dim=64, latent_dim=16).to(device)\n",
        "                    ood_optimizer = torch.optim.Adam(ood_detector.parameters(), lr=0.001)\n",
        "                    train_ood_detector(ood_detector, data, ood_optimizer, epochs=100)\n",
        "\n",
        "                    # Select nodes to poison\n",
        "                    poisoned_nodes = select_high_centrality_nodes(data, poisoned_node_budget)\n",
        "\n",
        "                    # Define attack methods\n",
        "                    attack_methods = ['SBA-Samp', 'SBA-Gen', 'GTA', 'UGBA', 'DPGBA']\n",
        "\n",
        "                    for attack in attack_methods:\n",
        "                        try:\n",
        "                            # Train with poisoned data\n",
        "                            trained_model, data_poisoned = train_with_poisoned_data(\n",
        "                                model=model,\n",
        "                                data=data,\n",
        "                                optimizer=optimizer,\n",
        "                                poisoned_nodes=poisoned_nodes,\n",
        "                                trigger_gen=trigger_gen,\n",
        "                                attack=attack,\n",
        "                                ood_detector=ood_detector,\n",
        "                                alpha=0.7,\n",
        "                                early_stopping=True\n",
        "                            )\n",
        "\n",
        "                            # Compute ASR and Clean Accuracy\n",
        "                            asr, clean_acc = compute_metrics(trained_model, data_poisoned, poisoned_nodes)\n",
        "                            results_summary.append({\n",
        "                                \"Dataset\": dataset_name,\n",
        "                                \"Model\": model_type,\n",
        "                                \"Attack\": attack,\n",
        "                                \"Defense\": \"None\",\n",
        "                                \"ASR\": asr,\n",
        "                                \"Clean Accuracy\": clean_acc\n",
        "                            })\n",
        "                            print(f\"Dataset: {dataset_name}, Model: {model_type}, Attack: {attack}, Defense: None - ASR: {asr:.2f}%, Clean Accuracy: {clean_acc:.2f}%\")\n",
        "\n",
        "                            # Defense 1: Dominant Set Outlier Detection (DSOD)\n",
        "                            pruned_nodes, data_poisoned_dsod = dominant_set_clustering(data_poisoned.clone(), threshold=0.9, use_pca=True, pca_components=10)\n",
        "                            asr_dsod, clean_acc_dsod = compute_metrics(trained_model, data_poisoned_dsod, poisoned_nodes)\n",
        "                            results_summary.append({\n",
        "                                \"Dataset\": dataset_name,\n",
        "                                \"Model\": model_type,\n",
        "                                \"Attack\": attack,\n",
        "                                \"Defense\": \"Dominant Set Outlier Detection\",\n",
        "                                \"ASR\": asr_dsod,\n",
        "                                \"Clean Accuracy\": clean_acc_dsod\n",
        "                            })\n",
        "                            print(f\"Dataset: {dataset_name}, Model: {model_type}, Attack: {attack}, Defense: DOMINANT - ASR: {asr_dsod:.2f}%, Clean Accuracy: {clean_acc_dsod:.2f}%\")\n",
        "\n",
        "                            # Defense 2: Prune\n",
        "                            data_poisoned_prune = defense_prune_edges(data_poisoned.clone(), quantile_threshold=0.8)\n",
        "                            asr_prune, clean_acc_prune = compute_metrics(trained_model, data_poisoned_prune, poisoned_nodes)\n",
        "                            results_summary.append({\n",
        "                                \"Dataset\": dataset_name,\n",
        "                                \"Model\": model_type,\n",
        "                                \"Attack\": attack,\n",
        "                                \"Defense\": \"Prune\",\n",
        "                                \"ASR\": asr_prune,\n",
        "                                \"Clean Accuracy\": clean_acc_prune\n",
        "                            })\n",
        "                            print(f\"Dataset: {dataset_name}, Model: {model_type}, Attack: {attack}, Defense: Prune - ASR: {asr_prune:.2f}%, Clean Accuracy: {clean_acc_prune:.2f}%\")\n",
        "\n",
        "                            # Defense 3: Prune + LD\n",
        "                            data_poisoned_prune_ld = defense_prune_and_discard_labels(data_poisoned.clone(), quantile_threshold=0.8)\n",
        "                            asr_prune_ld, clean_acc_prune_ld = compute_metrics(trained_model, data_poisoned_prune_ld, poisoned_nodes)\n",
        "                            results_summary.append({\n",
        "                                \"Dataset\": dataset_name,\n",
        "                                \"Model\": model_type,\n",
        "                                \"Attack\": attack,\n",
        "                                \"Defense\": \"Prune + LD\",\n",
        "                                \"ASR\": asr_prune_ld,\n",
        "                                \"Clean Accuracy\": clean_acc_prune_ld\n",
        "                            })\n",
        "                            print(f\"Dataset: {dataset_name}, Model: {model_type}, Attack: {attack}, Defense: Prune+LD - ASR: {asr_prune_ld:.2f}%, Clean Accuracy: {clean_acc_prune_ld:.2f}%\")\n",
        "\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error during attack {attack} on dataset {dataset_name} with model {model_type}: {e}\")\n",
        "                            continue\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error with model {model_type} on dataset {dataset_name}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with dataset {dataset_name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    results_df = pd.DataFrame(results_summary)\n",
        "    print(\"\\nSummary of Results:\")\n",
        "    print(results_df)\n",
        "    results_df.to_csv(\"backdoor_attack_results_summary.csv\", index=False)\n",
        "\n",
        "    # Visualize PCA projections for different attacks\n",
        "    visualize_pca_for_attacks(attack_embeddings_dict)\n",
        "\n",
        "# Run the function\n",
        "run_all_attacks()\n",
        "\n",
        "\n"
      ]
    }
  ]
}