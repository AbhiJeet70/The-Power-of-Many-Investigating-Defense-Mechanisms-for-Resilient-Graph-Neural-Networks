{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhiJeet70/The-Power-of-Many-Investigating-Defense-Mechanisms-for-Resilient-Graph-Neural-Networks/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayB58aQHQc66"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Install necessary packages\n",
        "!pip install torch-geometric\n",
        "!pip install matplotlib\n",
        "!pip install scikit-learn\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
        "from torch_geometric.datasets import Planetoid, Flickr\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "import networkx as nx\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import pandas as pd\n",
        "from torch_geometric.utils import to_networkx\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "\n",
        "set_seed()\n",
        "\n",
        "# Check if GPU is available and set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# Load datasets\n",
        "def load_dataset(dataset_name):\n",
        "    if dataset_name in [\"Cora\", \"PubMed\", \"CiteSeer\"]:\n",
        "        dataset = Planetoid(root=f\"./data/{dataset_name}\", name=dataset_name)\n",
        "    elif dataset_name == \"Flickr\":\n",
        "        dataset = Flickr(root=\"./data/Flickr\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Split dataset into train/validation/test\n",
        "# Updated to randomly mask out 20% of nodes, use 10% for labeled nodes, and 10% for validation\n",
        "def split_dataset(data, test_size=0.2, val_size=0.1):\n",
        "    num_nodes = data.num_nodes\n",
        "    indices = np.arange(num_nodes)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    num_test = int(test_size * num_nodes)\n",
        "    num_val = int(val_size * num_nodes)\n",
        "    num_train = num_nodes - num_test - num_val\n",
        "\n",
        "    train_mask = torch.zeros(num_nodes, dtype=torch.bool).to(device)\n",
        "    val_mask = torch.zeros(num_nodes, dtype=torch.bool).to(device)\n",
        "    test_mask = torch.zeros(num_nodes, dtype=torch.bool).to(device)\n",
        "\n",
        "    train_mask[indices[:num_train]] = True\n",
        "    val_mask[indices[num_train:num_train + num_val]] = True\n",
        "    test_mask[indices[num_train + num_val:]] = True\n",
        "\n",
        "    data.train_mask = train_mask\n",
        "    data.val_mask = val_mask\n",
        "    data.test_mask = test_mask\n",
        "\n",
        "    # Mask out 20% nodes for attack performance evaluation (half target, half clean test)\n",
        "    num_target = int(0.1 * num_nodes)  # Half of 20%\n",
        "    target_mask = torch.zeros(num_nodes, dtype=torch.bool).to(device)\n",
        "    clean_test_mask = torch.zeros(num_nodes, dtype=torch.bool).to(device)\n",
        "    target_mask[indices[num_train + num_val:num_train + num_val + num_target]] = True\n",
        "    clean_test_mask[indices[num_train + num_val + num_target:]] = True\n",
        "\n",
        "    data.target_mask = target_mask\n",
        "    data.clean_test_mask = clean_test_mask\n",
        "\n",
        "    return data\n",
        "\n",
        "# Define GNN Model with multiple architectures (GCN, GraphSAGE, GAT)\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, model_type='GCN'):\n",
        "        super(GNN, self).__init__()\n",
        "        if model_type == 'GCN':\n",
        "            self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "            self.conv2 = GCNConv(hidden_dim, output_dim)\n",
        "        elif model_type == 'GraphSage':\n",
        "            self.conv1 = SAGEConv(input_dim, hidden_dim)\n",
        "            self.conv2 = SAGEConv(hidden_dim, output_dim)\n",
        "        elif model_type == 'GAT':\n",
        "            self.conv1 = GATConv(input_dim, hidden_dim, heads=8, concat=True)\n",
        "            self.conv2 = GATConv(hidden_dim * 8, output_dim, heads=1, concat=False)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Select nodes to poison based on high-centrality (degree centrality) for a stronger impact\n",
        "def select_high_centrality_nodes(data, num_nodes_to_select):\n",
        "    graph = nx.Graph()\n",
        "    edge_index = data.edge_index.cpu().numpy()\n",
        "    graph.add_edges_from(edge_index.T)\n",
        "    centrality = nx.degree_centrality(graph)\n",
        "    sorted_nodes = sorted(centrality, key=centrality.get, reverse=True)\n",
        "    return torch.tensor(sorted_nodes[:num_nodes_to_select], dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "class TriggerGenerator(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(TriggerGenerator, self).__init__()\n",
        "        self.mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_dim, hidden_dim),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hidden_dim, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "class OODDetector(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(OODDetector, self).__init__()\n",
        "        # Encoder\n",
        "        self.encoder = torch.nn.Sequential(\n",
        "            GCNConv(input_dim, hidden_dim),\n",
        "            torch.nn.ReLU(),\n",
        "            GCNConv(hidden_dim, latent_dim),\n",
        "        )\n",
        "        # Decoder\n",
        "        self.decoder = torch.nn.Sequential(\n",
        "            torch.nn.Linear(latent_dim, hidden_dim),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hidden_dim, input_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # Encode\n",
        "        z = self.encoder[0](x, edge_index)\n",
        "        z = self.encoder[1](z)\n",
        "        z = self.encoder[2](z, edge_index)\n",
        "\n",
        "        # Decode\n",
        "        reconstructed_x = self.decoder(z)\n",
        "        return reconstructed_x, z\n",
        "\n",
        "    def reconstruction_loss(self, x, edge_index):\n",
        "        reconstructed_x, _ = self.forward(x, edge_index)\n",
        "        loss = F.mse_loss(reconstructed_x, x, reduction='none').mean(dim=1)\n",
        "        return loss\n",
        "\n",
        "def train_ood_detector(ood_detector, data, optimizer, epochs=50):\n",
        "    ood_detector.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()  # Clear the gradients\n",
        "\n",
        "        # Forward pass\n",
        "        reconstructed_x, _ = ood_detector(data.x, data.edge_index)\n",
        "\n",
        "        # Use only the training mask to compute reconstruction loss\n",
        "        loss = F.mse_loss(reconstructed_x[data.train_mask], data.x[data.train_mask])\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        # Print loss every 10 epochs\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Reconstruction Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "# Training Function with Poisoned Data\n",
        "def train_with_poisoned_data(model, data, optimizer, poisoned_nodes, trigger_gen, attack, ood_detector=None, alpha=0.7, early_stopping=False):\n",
        "    # Apply trigger injection\n",
        "    data_poisoned = inject_trigger(data, poisoned_nodes, attack, trigger_gen, ood_detector=ood_detector, alpha=0.7)\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    for epoch in range(100):\n",
        "        optimizer.zero_grad()  # Clear the gradients\n",
        "\n",
        "        # Forward pass\n",
        "        out = model(data_poisoned.x, data_poisoned.edge_index)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = F.cross_entropy(out[data_poisoned.train_mask], data_poisoned.y[data_poisoned.train_mask])\n",
        "\n",
        "        # Backward pass\n",
        "        # Ensure we only retain the graph if we need to perform multiple backward passes\n",
        "        if epoch < 99:  # In all but the last epoch, retain the graph\n",
        "            loss.backward(retain_graph=True)\n",
        "        else:\n",
        "            loss.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Optional: Print loss during training for insight\n",
        "        if early_stopping and epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    return model, data_poisoned\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "class GCNEncoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GCNEncoder, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
        "        self.conv2 = GCNConv(2 * out_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "def select_diverse_nodes(data, num_nodes_to_select, num_clusters=None):\n",
        "    \"\"\"\n",
        "    Select nodes using a clustering-based approach to ensure diversity, along with high-degree nodes.\n",
        "\n",
        "    Parameters:\n",
        "    - data: PyG data object representing the graph.\n",
        "    - num_nodes_to_select: Number of nodes to select for poisoning.\n",
        "    - num_clusters: Number of clusters to form for diversity. Defaults to number of classes if not provided.\n",
        "\n",
        "    Returns:\n",
        "    - Tensor containing indices of selected nodes.\n",
        "    \"\"\"\n",
        "    if num_clusters is None:\n",
        "        num_clusters = data.num_features  # Use the number of features as the default number of clusters\n",
        "\n",
        "    # Node feature embeddings\n",
        "    node_features = data.x.cpu().numpy()\n",
        "\n",
        "    # Use GCN encoder to get node embeddings that capture both attribute and structural information\n",
        "    encoder = GCNEncoder(data.num_features, out_channels=16)  # Assuming out_channels = 16\n",
        "    encoder.eval()\n",
        "    with torch.no_grad():\n",
        "        embeddings = encoder(data.x, data.edge_index).cpu().numpy()\n",
        "\n",
        "    # Perform K-means clustering to find representative nodes\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(embeddings)\n",
        "    labels = kmeans.labels_\n",
        "    cluster_centers = kmeans.cluster_centers_\n",
        "\n",
        "    # Select nodes closest to the cluster centers\n",
        "    selected_nodes = []\n",
        "    for i in range(num_clusters):\n",
        "        cluster_indices = np.where(labels == i)[0]\n",
        "        center = cluster_centers[i]\n",
        "        distances = np.linalg.norm(embeddings[cluster_indices] - center, axis=1)\n",
        "        closest_node = cluster_indices[np.argmin(distances)]\n",
        "        selected_nodes.append(closest_node)\n",
        "\n",
        "    # Calculate node degrees\n",
        "    degree = torch.bincount(data.edge_index[0])  # Calculate node degrees\n",
        "    # Select high-degree nodes\n",
        "    high_degree_nodes = torch.topk(degree, len(selected_nodes) // 2).indices\n",
        "\n",
        "    # Convert the graph to NetworkX to calculate centrality measures\n",
        "    G = to_networkx(data, to_undirected=True)\n",
        "    betweenness_centrality = nx.betweenness_centrality(G)\n",
        "    central_nodes = sorted(betweenness_centrality, key=betweenness_centrality.get, reverse=True)\n",
        "    central_nodes_tensor = torch.tensor(central_nodes[:len(selected_nodes) // 2], dtype=torch.long)\n",
        "\n",
        "    # Combine diverse nodes, high-degree nodes, and central nodes\n",
        "    combined_nodes = torch.cat([torch.tensor(selected_nodes), high_degree_nodes, central_nodes_tensor])\n",
        "    # Get unique nodes and limit to num_nodes_to_select\n",
        "    unique_nodes = torch.unique(combined_nodes)[:num_nodes_to_select]\n",
        "\n",
        "    return torch.tensor(selected_nodes[:num_nodes_to_select], dtype=torch.long).to(data.x.device)\n",
        "\n",
        "def inject_trigger(data, poisoned_nodes, attack_type, trigger_gen=None, ood_detector=None, alpha=0.7, trigger_size=5, trigger_density=0.5, input_dim=None):\n",
        "    data_poisoned = data.clone()\n",
        "\n",
        "    if poisoned_nodes.numel() < trigger_size:\n",
        "        raise ValueError(f\"Insufficient poisoned nodes: required {trigger_size}, found {poisoned_nodes.numel()}\")\n",
        "\n",
        "    if attack_type == 'SBA-Samp':\n",
        "        G = nx.erdos_renyi_graph(trigger_size, trigger_density)\n",
        "        trigger_edge_index = torch.tensor(list(G.edges), dtype=torch.long).t().contiguous()\n",
        "\n",
        "        if trigger_edge_index.numel() > 0:\n",
        "            # Map local indices to global indices using poisoned_nodes\n",
        "            trigger_edge_index = torch.cat([\n",
        "                poisoned_nodes[trigger_edge_index[0]].unsqueeze(0),\n",
        "                poisoned_nodes[trigger_edge_index[1]].unsqueeze(0)\n",
        "            ], dim=0)\n",
        "\n",
        "        # Randomly connect poisoned nodes to existing graph nodes\n",
        "        poisoned_edges = torch.stack([\n",
        "            poisoned_nodes[:trigger_size],\n",
        "            torch.randint(0, data.num_nodes, (trigger_size,), device=device)\n",
        "        ])\n",
        "\n",
        "        # Update edge index\n",
        "        data_poisoned.edge_index = torch.cat([data.edge_index, trigger_edge_index.to(device), poisoned_edges.to(device)], dim=1)\n",
        "\n",
        "        # Generate new features for poisoned nodes\n",
        "        avg_features = torch.stack([\n",
        "            data.x[data.edge_index[0][data.edge_index[1] == node]].mean(dim=0) if len(data.edge_index[0][data.edge_index[1] == node]) > 0 else data.x.mean(dim=0)\n",
        "            for node in poisoned_nodes[:trigger_size]\n",
        "        ])\n",
        "        data_poisoned.x[poisoned_nodes[:trigger_size]] = avg_features + torch.randn_like(avg_features) * 0.02\n",
        "\n",
        "    elif attack_type == 'SBA-Gen':\n",
        "        G = nx.erdos_renyi_graph(trigger_size, trigger_density)\n",
        "        trigger_edge_index = torch.tensor(list(G.edges), dtype=torch.long).t().contiguous()\n",
        "\n",
        "        if trigger_edge_index.numel() > 0:\n",
        "            # Map local indices to global indices using poisoned_nodes\n",
        "            trigger_edge_index = torch.cat([\n",
        "                poisoned_nodes[trigger_edge_index[0]].unsqueeze(0),\n",
        "                poisoned_nodes[trigger_edge_index[1]].unsqueeze(0)\n",
        "            ], dim=0)\n",
        "\n",
        "        # Randomly connect poisoned nodes to existing graph nodes\n",
        "        poisoned_edges = torch.stack([\n",
        "            poisoned_nodes[:trigger_size],\n",
        "            torch.randint(0, data.num_nodes, (trigger_size,), device=device)\n",
        "        ])\n",
        "\n",
        "        # Update edge index\n",
        "        data_poisoned.edge_index = torch.cat([data.edge_index, trigger_edge_index.to(device), poisoned_edges.to(device)], dim=1)\n",
        "\n",
        "        # Generate Gaussian-distributed features for poisoned nodes\n",
        "        avg_features = torch.stack([\n",
        "            data.x[data.edge_index[0][data.edge_index[1] == node]].mean(dim=0) if len(data.edge_index[0][data.edge_index[1] == node]) > 0 else data.x.mean(dim=0)\n",
        "            for node in poisoned_nodes[:trigger_size]\n",
        "        ])\n",
        "        data_poisoned.x[poisoned_nodes[:trigger_size]] = avg_features + torch.normal(mean=0.0, std=0.03, size=avg_features.shape).to(data.x.device)\n",
        "\n",
        "    elif attack_type == 'DPGBA':\n",
        "        if ood_detector is None:\n",
        "            raise ValueError(\"OODDetector must be provided for DPGBA attack.\")\n",
        "\n",
        "        # Use OODDetector to refine trigger features\n",
        "        ood_detector.eval()\n",
        "        with torch.no_grad():\n",
        "            _, latent_embeddings = ood_detector(data.x, data.edge_index)\n",
        "\n",
        "        poisoned_latent_embeddings = latent_embeddings[poisoned_nodes]\n",
        "        refined_trigger_latent = poisoned_latent_embeddings + torch.randn_like(poisoned_latent_embeddings) * 0.1\n",
        "\n",
        "        # Map latent embeddings back to feature space using the decoder\n",
        "        refined_trigger_features = ood_detector.decoder(refined_trigger_latent)\n",
        "\n",
        "        # Interpolate with existing features for in-distribution preservation\n",
        "        node_alphas = torch.rand(len(poisoned_nodes)).to(data.x.device) * 0.3 + 0.5\n",
        "        distribution_preserved_features = (\n",
        "            node_alphas.unsqueeze(1) * data.x[poisoned_nodes]\n",
        "            + (1 - node_alphas.unsqueeze(1)) * refined_trigger_features\n",
        "        )\n",
        "\n",
        "        data_poisoned.x[poisoned_nodes] = distribution_preserved_features\n",
        "\n",
        "    elif attack_type == 'GTA':\n",
        "        connected_nodes = [data.edge_index[0][data.edge_index[1] == node] for node in poisoned_nodes]\n",
        "        avg_features = torch.stack([\n",
        "            data.x[nodes].mean(dim=0) if len(nodes) > 0 else data.x.mean(dim=0) for nodes in connected_nodes\n",
        "        ])\n",
        "        trigger_features = avg_features + torch.randn_like(avg_features) * 0.05\n",
        "        data_poisoned.x[poisoned_nodes] = trigger_features\n",
        "\n",
        "    elif attack_type == 'UGBA':\n",
        "        diverse_nodes = select_diverse_nodes(data_poisoned, len(poisoned_nodes))\n",
        "        connected_nodes = [data_poisoned.edge_index[0][data_poisoned.edge_index[1] == node] for node in diverse_nodes]\n",
        "\n",
        "        avg_features = torch.stack([\n",
        "            data_poisoned.x[nodes].mean(dim=0) if len(nodes) > 0 else data_poisoned.x.mean(dim=0) for nodes in connected_nodes\n",
        "        ])\n",
        "        refined_trigger_features = avg_features + torch.normal(mean=2.0, std=0.5, size=avg_features.shape).to(data_poisoned.x.device)\n",
        "        data_poisoned.x[diverse_nodes] = refined_trigger_features\n",
        "\n",
        "        new_edges = []\n",
        "        for i in range(len(diverse_nodes)):\n",
        "            node = diverse_nodes[i]\n",
        "            neighbor = connected_nodes[i][0] if len(connected_nodes[i]) > 0 else diverse_nodes[(i + 1) % len(diverse_nodes)]\n",
        "            new_edges.append([node, neighbor])\n",
        "\n",
        "        new_edges = torch.tensor(new_edges, dtype=torch.long).t().contiguous().to(data_poisoned.edge_index.device)\n",
        "        data_poisoned.edge_index = torch.cat([data_poisoned.edge_index, new_edges], dim=1)\n",
        "\n",
        "    return data_poisoned\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import gc\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "def dominant_set_clustering(data, threshold=0.7, use_pca=True, pca_components=10):\n",
        "    \"\"\"\n",
        "    Applies a robust outlier detection framework using K-Means clustering and distance-based heuristics.\n",
        "\n",
        "    Parameters:\n",
        "    - data: PyG data object representing the graph.\n",
        "    - threshold: Quantile threshold for identifying outliers based on cluster distances.\n",
        "    - use_pca: Whether to use PCA for dimensionality reduction.\n",
        "    - pca_components: Number of PCA components to use if PCA is applied.\n",
        "\n",
        "    Returns:\n",
        "    - pruned_nodes: Set of nodes identified as outliers.\n",
        "    - data: Updated PyG data object with modified features and labels for outliers.\n",
        "    \"\"\"\n",
        "    # Step 1: Dimensionality reduction using PCA (optional)\n",
        "    node_features = data.x.detach().cpu().numpy()\n",
        "    if use_pca and node_features.shape[1] > pca_components:\n",
        "        pca = PCA(n_components=pca_components)\n",
        "        node_features = pca.fit_transform(node_features)\n",
        "\n",
        "    # Step 2: Determine the number of clusters dynamically\n",
        "    num_classes = len(torch.unique(data.y).tolist())  # Number of unique classes\n",
        "    num_nodes = node_features.shape[0]  # Total number of nodes\n",
        "    n_clusters = min(num_classes, num_nodes)\n",
        "\n",
        "    # Step 3: K-Means Clustering to identify clusters and potential outliers\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(node_features)\n",
        "    cluster_labels = kmeans.labels_\n",
        "    cluster_centers = kmeans.cluster_centers_\n",
        "\n",
        "    # Calculate distances to cluster centers\n",
        "    distances = np.linalg.norm(node_features - cluster_centers[cluster_labels], axis=1)\n",
        "\n",
        "    # Identify outlier candidates based on distance threshold\n",
        "    distance_threshold = np.percentile(distances, 100 * threshold)\n",
        "    outlier_candidates = np.where(distances > distance_threshold)[0]\n",
        "\n",
        "    # Step 4: Update data to reflect removal of outlier influence\n",
        "    pruned_nodes = set(outlier_candidates)\n",
        "    if len(pruned_nodes) > 0:\n",
        "        outliers = torch.tensor(list(pruned_nodes), dtype=torch.long, device=data.x.device)\n",
        "\n",
        "        # Assign an invalid label (-1) to outlier nodes to discard them during training\n",
        "        data.y[outliers] = -1\n",
        "\n",
        "        # Replace the features of outliers with the average feature value to reduce their impact\n",
        "        data.x[outliers] = data.x.mean(dim=0).to(data.x.device)\n",
        "\n",
        "    return pruned_nodes, data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def defense_prune_edges(data, quantile_threshold=0.9):\n",
        "    \"\"\"\n",
        "    Prunes edges based on adaptive cosine similarity between node features.\n",
        "\n",
        "    Parameters:\n",
        "    - data: PyG data object representing the graph.\n",
        "    - quantile_threshold: Quantile to determine pruning threshold (e.g., 0.9 means pruning edges in the top 10% dissimilar).\n",
        "\n",
        "    Returns:\n",
        "    - data: Updated PyG data object with pruned edges.\n",
        "    \"\"\"\n",
        "    features = data.x\n",
        "    norm_features = F.normalize(features, p=2, dim=1)  # Normalize features\n",
        "    edge_index = data.edge_index\n",
        "\n",
        "    # Calculate cosine similarity for each edge\n",
        "    src, dst = edge_index[0], edge_index[1]\n",
        "    cosine_similarities = torch.sum(norm_features[src] * norm_features[dst], dim=1)\n",
        "\n",
        "    # Adaptive threshold based on quantile of similarity distribution\n",
        "    similarity_threshold = torch.quantile(cosine_similarities, quantile_threshold).item()\n",
        "\n",
        "    # Keep edges with cosine similarity above the threshold\n",
        "    pruned_mask = cosine_similarities >= similarity_threshold\n",
        "    pruned_edges = edge_index[:, pruned_mask]\n",
        "\n",
        "    # Update edge index with pruned edges\n",
        "    data.edge_index = pruned_edges\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def defense_prune_and_discard_labels(data, quantile_threshold=0.2):\n",
        "    \"\"\"\n",
        "    Prunes edges based on adaptive cosine similarity and discards labels of nodes connected by pruned edges selectively.\n",
        "\n",
        "    Parameters:\n",
        "    - data: PyG data object representing the graph.\n",
        "    - quantile_threshold: Quantile threshold for cosine similarity pruning (e.g., 0.2 means pruning edges in the bottom 20%).\n",
        "\n",
        "    Returns:\n",
        "    - data: Updated PyG data object with pruned edges and selectively discarded labels.\n",
        "    \"\"\"\n",
        "    features = data.x\n",
        "    norm_features = F.normalize(features, p=2, dim=1)  # Normalize features using PyTorch\n",
        "    edge_index = data.edge_index\n",
        "\n",
        "    # Calculate cosine similarity for each edge\n",
        "    src, dst = edge_index[0], edge_index[1]\n",
        "    cosine_similarities = torch.sum(norm_features[src] * norm_features[dst], dim=1)\n",
        "\n",
        "    # Use quantile to determine adaptive threshold for pruning\n",
        "    adaptive_threshold = torch.quantile(cosine_similarities, quantile_threshold).item()\n",
        "\n",
        "    # Mask edges with similarity below the adaptive threshold\n",
        "    pruned_mask = cosine_similarities < adaptive_threshold\n",
        "    pruned_edges = edge_index[:, ~pruned_mask]  # Retain edges that are above the threshold\n",
        "\n",
        "    # Update edge index with pruned edges\n",
        "    data.edge_index = pruned_edges\n",
        "\n",
        "    # Selectively discard labels of nodes connected by many pruned edges\n",
        "    pruned_src, pruned_dst = edge_index[:, pruned_mask]\n",
        "    pruned_nodes_count = torch.bincount(torch.cat([pruned_src, pruned_dst]), minlength=data.num_nodes)\n",
        "\n",
        "    # Only discard labels if the node has a high count of pruned edges\n",
        "    threshold_count = int(torch.median(pruned_nodes_count).item())  # Use median count as a threshold\n",
        "    nodes_to_discard = torch.where(pruned_nodes_count > threshold_count)[0]\n",
        "\n",
        "    data.y[nodes_to_discard] = -1  # Use -1 to represent discarded labels\n",
        "\n",
        "    return data\n",
        "\n",
        "# Compute ASR and Clean Accuracy (using .detach() to avoid retaining computation graph)\n",
        "def compute_metrics(model, data, poisoned_nodes):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data.x, data.edge_index).detach()\n",
        "        _, pred = out.max(dim=1)\n",
        "        asr = (pred[poisoned_nodes] == data.y[poisoned_nodes]).sum().item() / len(poisoned_nodes) * 100\n",
        "        clean_acc = accuracy_score(data.y[data.test_mask].cpu(), pred[data.test_mask].cpu()) * 100\n",
        "    return asr, clean_acc\n",
        "\n",
        "\n",
        "\n",
        "# Visualization Function\n",
        "# Visualize PCA for Attacks\n",
        "# Added function to visualize PCA projections of node embeddings for different attacks\n",
        "def visualize_pca_for_attacks(attack_embeddings_dict):\n",
        "    pca = PCA(n_components=2)\n",
        "    plt.figure(figsize=(20, 10))\n",
        "\n",
        "    for i, (attack, attack_data) in enumerate(attack_embeddings_dict.items(), 1):\n",
        "        embeddings = attack_data['data'].detach().cpu().numpy()\n",
        "        poisoned_nodes = attack_data['poisoned_nodes'].detach().cpu().numpy()\n",
        "\n",
        "        # Apply PCA to the node embeddings\n",
        "        pca_result = pca.fit_transform(embeddings)\n",
        "\n",
        "        # Create masks for clean and poisoned nodes\n",
        "        clean_mask = np.ones(embeddings.shape[0], dtype=bool)\n",
        "        clean_mask[poisoned_nodes] = False\n",
        "\n",
        "        # Extract clean and poisoned node embeddings after PCA\n",
        "        clean_embeddings = pca_result[clean_mask]\n",
        "        poisoned_embeddings = pca_result[~clean_mask]\n",
        "\n",
        "        # Plotting clean and poisoned nodes\n",
        "        plt.subplot(2, 3, i)\n",
        "        plt.scatter(clean_embeddings[:, 0], clean_embeddings[:, 1], s=10, alpha=0.5, label='Clean Nodes', c='b')\n",
        "        plt.scatter(poisoned_embeddings[:, 0], poisoned_embeddings[:, 1], s=10, alpha=0.8, label='Poisoned Nodes', c='r')\n",
        "        plt.title(f'PCA Visualization for {attack}')\n",
        "        plt.xlabel('PCA Component 1')\n",
        "        plt.ylabel('PCA Component 2')\n",
        "        plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Run all attacks and apply defenses\n",
        "def run_all_attacks():\n",
        "    datasets = [\"Cora\", \"PubMed\", \"CiteSeer\"]\n",
        "    results_summary = []\n",
        "    attack_embeddings_dict = {}\n",
        "\n",
        "    for dataset_name in datasets:\n",
        "        dataset = load_dataset(dataset_name)\n",
        "        data = dataset[0].to(device)\n",
        "        input_dim = data.num_features\n",
        "        output_dim = dataset.num_classes if isinstance(dataset.num_classes, int) else dataset.num_classes[0]\n",
        "        data = split_dataset(data)\n",
        "\n",
        "        # Dataset-specific poisoning budgets\n",
        "        dataset_budgets = {\n",
        "            'Cora': 10,\n",
        "            'PubMed': 40,\n",
        "            'CiteSeer': 30\n",
        "        }\n",
        "        poisoned_node_budget = dataset_budgets.get(dataset_name, 10)\n",
        "\n",
        "        # Define GNN models for experiments\n",
        "        model_types = ['GCN', 'GraphSage', 'GAT']\n",
        "\n",
        "        for model_type in model_types:\n",
        "            # Initialize model and optimizer\n",
        "            model = GNN(input_dim=input_dim, hidden_dim=64, output_dim=output_dim, model_type=model_type).to(device)\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
        "\n",
        "            # Evaluate baseline accuracy without attack or defense\n",
        "            model.train()\n",
        "            baseline_epochs = 200  # Number of epochs for baseline training\n",
        "            for epoch in range(baseline_epochs):\n",
        "                optimizer.zero_grad()\n",
        "                out = model(data.x, data.edge_index)\n",
        "                loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Evaluate baseline accuracy\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                out = model(data.x, data.edge_index)\n",
        "                predictions = out.argmax(dim=1)\n",
        "                baseline_acc = (predictions[data.test_mask] == data.y[data.test_mask]).sum().item() / data.test_mask.sum().item()\n",
        "                baseline_acc_percentage = baseline_acc * 100\n",
        "                print(f\"Dataset: {dataset_name}, Model: {model_type}, Baseline Accuracy: {baseline_acc_percentage:.2f}%\")\n",
        "\n",
        "                # Record baseline accuracy in results\n",
        "                results_summary.append({\n",
        "                    \"Dataset\": dataset_name,\n",
        "                    \"Model\": model_type,\n",
        "                    \"Attack\": \"None\",\n",
        "                    \"Defense\": \"None\",\n",
        "                    \"ASR\": \"N/A\",\n",
        "                    \"Clean Accuracy\": baseline_acc_percentage\n",
        "                })\n",
        "\n",
        "            # Initialize Trigger Generator and OOD Detector for attacks\n",
        "            trigger_gen = TriggerGenerator(input_dim=input_dim, hidden_dim=64).to(device)\n",
        "            ood_detector = OODDetector(input_dim=input_dim, hidden_dim=64, latent_dim=16).to(device)\n",
        "\n",
        "            # Train OOD detector\n",
        "            ood_optimizer = torch.optim.Adam(ood_detector.parameters(), lr=0.001)\n",
        "            train_ood_detector(ood_detector, data, ood_optimizer)\n",
        "\n",
        "            # Select nodes to poison based on high-centrality (degree centrality) for a stronger impact\n",
        "            poisoned_nodes = select_high_centrality_nodes(data, poisoned_node_budget)\n",
        "\n",
        "            # Define different attacks\n",
        "            attack_methods = ['SBA-Samp', 'SBA-Gen', 'GTA', 'UGBA', 'DPGBA']\n",
        "\n",
        "            for attack in attack_methods:\n",
        "                # Train model with poisoned data\n",
        "                if attack == 'DPGBA':\n",
        "                    trained_model, data_poisoned = train_with_poisoned_data(\n",
        "                        model=model,\n",
        "                        data=data,\n",
        "                        optimizer=optimizer,\n",
        "                        poisoned_nodes=poisoned_nodes,\n",
        "                        trigger_gen=trigger_gen,\n",
        "                        attack=attack,\n",
        "                        ood_detector=ood_detector,\n",
        "                        alpha=0.7,\n",
        "                        early_stopping=True\n",
        "                    )\n",
        "                else:\n",
        "                    trained_model, data_poisoned = train_with_poisoned_data(\n",
        "                        model=model,\n",
        "                        data=data,\n",
        "                        optimizer=optimizer,\n",
        "                        poisoned_nodes=poisoned_nodes,\n",
        "                        trigger_gen=trigger_gen,\n",
        "                        attack=attack,\n",
        "                        alpha=0.7,\n",
        "                        early_stopping=True\n",
        "                    )\n",
        "\n",
        "                # Compute ASR and Clean Accuracy before applying any defense\n",
        "                asr, clean_acc = compute_metrics(trained_model, data_poisoned, poisoned_nodes)\n",
        "                results_summary.append({\n",
        "                    \"Dataset\": dataset_name,\n",
        "                    \"Model\": model_type,\n",
        "                    \"Attack\": attack,\n",
        "                    \"Defense\": \"None\",\n",
        "                    \"ASR\": asr,\n",
        "                    \"Clean Accuracy\": clean_acc\n",
        "                })\n",
        "                print(f\"Dataset: {dataset_name}, Model: {model_type}, Attack: {attack}, Defense: None - ASR: {asr:.2f}%, Clean Accuracy: {clean_acc:.2f}%\")\n",
        "\n",
        "                # Apply defenses\n",
        "                # Defense 1: Dominant Set Outlier Detection (DSOD)\n",
        "                pruned_nodes, data_poisoned_dsod = dominant_set_clustering(data_poisoned.clone(), threshold=0.9, use_pca=True, pca_components=10)\n",
        "                asr_dsod, clean_acc_dsod = compute_metrics(trained_model, data_poisoned_dsod, poisoned_nodes)\n",
        "                results_summary.append({\n",
        "                    \"Dataset\": dataset_name,\n",
        "                    \"Model\": model_type,\n",
        "                    \"Attack\": attack,\n",
        "                    \"Defense\": \"Dominant Set Outlier Detection\",\n",
        "                    \"ASR\": asr_dsod,\n",
        "                    \"Clean Accuracy\": clean_acc_dsod\n",
        "                })\n",
        "                print(f\"Dataset: {dataset_name}, Model: {model_type}, Attack: {attack}, Defense: Dominant Set Outlier Detection - ASR: {asr_dsod:.2f}%, Clean Accuracy: {clean_acc_dsod:.2f}%\")\n",
        "\n",
        "                # Defense 2: Prune\n",
        "                data_poisoned_prune = defense_prune_edges(data_poisoned.clone(), quantile_threshold=0.8)\n",
        "                asr_prune, clean_acc_prune = compute_metrics(trained_model, data_poisoned_prune, poisoned_nodes)\n",
        "                results_summary.append({\n",
        "                    \"Dataset\": dataset_name,\n",
        "                    \"Model\": model_type,\n",
        "                    \"Attack\": attack,\n",
        "                    \"Defense\": \"Prune\",\n",
        "                    \"ASR\": asr_prune,\n",
        "                    \"Clean Accuracy\": clean_acc_prune\n",
        "                })\n",
        "                print(f\"Dataset: {dataset_name}, Model: {model_type}, Attack: {attack}, Defense: Prune - ASR: {asr_prune:.2f}%, Clean Accuracy: {clean_acc_prune:.2f}%\")\n",
        "\n",
        "                # Defense 3: Prune + LD\n",
        "                data_poisoned_prune_ld = defense_prune_and_discard_labels(data_poisoned.clone(), quantile_threshold=0.8)\n",
        "                asr_prune_ld, clean_acc_prune_ld = compute_metrics(trained_model, data_poisoned_prune_ld, poisoned_nodes)\n",
        "                results_summary.append({\n",
        "                    \"Dataset\": dataset_name,\n",
        "                    \"Model\": model_type,\n",
        "                    \"Attack\": attack,\n",
        "                    \"Defense\": \"Prune + LD\",\n",
        "                    \"ASR\": asr_prune_ld,\n",
        "                    \"Clean Accuracy\": clean_acc_prune_ld\n",
        "                })\n",
        "                print(f\"Dataset: {dataset_name}, Model: {model_type}, Attack: {attack}, Defense: Prune + LD - ASR: {asr_prune_ld:.2f}%, Clean Accuracy: {clean_acc_prune_ld:.2f}%\")\n",
        "\n",
        "                # Store embeddings for visualization\n",
        "                attack_embeddings_dict[f\"{dataset_name}-{model_type}-{attack}\"] = {\n",
        "                    'data': data_poisoned_dsod.x,\n",
        "                    'poisoned_nodes': poisoned_nodes\n",
        "                }\n",
        "\n",
        "                # Clear memory after each attack-defense cycle\n",
        "                variables_to_clear = ['model', 'optimizer', 'trigger_gen', 'ood_detector', 'data_poisoned', 'trained_model']\n",
        "                for var_name in variables_to_clear:\n",
        "                    if var_name in locals():\n",
        "                        del locals()[var_name]\n",
        "\n",
        "                # Release memory\n",
        "                gc.collect()\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "    # Summarize Results in a Table\n",
        "    results_df = pd.DataFrame(results_summary)\n",
        "    print(\"\\nSummary of Attack Success Rate and Clean Accuracy Before and After Defenses:\")\n",
        "    print(results_df)\n",
        "\n",
        "    results_df.to_csv(\"backdoor_attack_results_summary.csv\", index=False)\n",
        "\n",
        "    # Visualize PCA projections for different attacks\n",
        "    visualize_pca_for_attacks(attack_embeddings_dict)\n",
        "\n",
        "# Run the function\n",
        "run_all_attacks()\n",
        "\n",
        "# Download the results\n",
        "from google.colab import files\n",
        "files.download(\"backdoor_attack_results_summary.csv\")\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30787,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}